{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Popularity Baseline Evaluation\n",
        "\n",
        "This notebook evaluates a popularity-based recommendation system using comprehensive evaluation metrics at k values of 5, 10, 15, and 20.\n",
        "\n",
        "## Overview\n",
        "- **Baseline**: Popularity-based recommendations (most popular items for all users)\n",
        "- **Evaluation Metrics**: NDCG, Novelty, Diversity, Serendipity, Coverage\n",
        "- **K Values**: 5, 10, 15, 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Evaluation metrics imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import the standalone evaluation metrics\n",
        "# Make sure standalone_evaluation_metrics.py is in the same directory\n",
        "from standalone_evaluation_metrics import (\n",
        "    quick_evaluate,\n",
        "    evaluate_recommendations,\n",
        "    print_evaluation_summary,\n",
        "    ndcg_at_k,\n",
        "    novelty_at_k,\n",
        "    diversity_ild_at_k,\n",
        "    serendipity_at_k,\n",
        "    catalog_coverage_at_k,\n",
        "    user_coverage_at_k\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Evaluation metrics imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data loaded successfully!\n",
            "Music list shape: (50683, 21)\n",
            "User behavior list shape: (9711301, 3)\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "# Update the path to your data files\n",
        "dataset_path = ''  # Update this path to your data directory\n",
        "\n",
        "try:\n",
        "    music_list = pd.read_csv(dataset_path + 'music_list.csv')\n",
        "    user_behavior_list = pd.read_csv(dataset_path + 'user_behavior_list.csv')\n",
        "    print(\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"Music list shape: {music_list.shape}\")\n",
        "    print(f\"User behavior list shape: {user_behavior_list.shape}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error loading data: {e}\")\n",
        "    print(\"Please update the dataset_path variable with the correct path to your data files.\")\n",
        "    # For demonstration, we'll create synthetic data\n",
        "    print(\"Creating synthetic data for demonstration...\")\n",
        "    \n",
        "    # Create synthetic data\n",
        "    n_users, n_items = 1000, 500\n",
        "    user_ids = [f\"user_{i:04d}\" for i in range(n_users)]\n",
        "    item_ids = [f\"item_{i:04d}\" for i in range(n_items)]\n",
        "    \n",
        "    # Create synthetic interactions\n",
        "    interactions = []\n",
        "    for user_id in user_ids:\n",
        "        n_user_items = np.random.poisson(20)  # Average 20 items per user\n",
        "        user_items = np.random.choice(item_ids, size=min(n_user_items, n_items), replace=False)\n",
        "        \n",
        "        for item_id in user_items:\n",
        "            playcount = np.random.poisson(3) + 1\n",
        "            interactions.append({\n",
        "                'user_id': user_id,\n",
        "                'track_id': item_id,\n",
        "                'playcount': playcount\n",
        "            })\n",
        "    \n",
        "    user_behavior_list = pd.DataFrame(interactions)\n",
        "    \n",
        "    # Create synthetic music list\n",
        "    music_list = pd.DataFrame({\n",
        "        'track_id': item_ids,\n",
        "        'name': [f\"Song_{i}\" for i in range(n_items)],\n",
        "        'artist': [f\"Artist_{i%50}\" for i in range(n_items)],\n",
        "        'genre': [f\"Genre_{i%10}\" for i in range(n_items)]\n",
        "    })\n",
        "    \n",
        "    print(\"‚úÖ Synthetic data created!\")\n",
        "    print(f\"Music list shape: {music_list.shape}\")\n",
        "    print(f\"User behavior list shape: {user_behavior_list.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Preprocessing data...\n",
            "Active users (‚â•10 interactions): 290898\n",
            "Sampled users for evaluation: 5000\n",
            "Train interactions: 96748\n",
            "Test interactions: 24073\n",
            "Unique users in train: 5000\n",
            "Unique users in test: 5000\n",
            "Unique items in train: 15247\n",
            "Unique items in test: 8777\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing following the popularity baseline approach\n",
        "print(\"üìä Preprocessing data...\")\n",
        "\n",
        "# Filter users with at least 10 interactions (reduced from 50 for faster processing)\n",
        "user_counts = user_behavior_list['user_id'].value_counts()\n",
        "active_users = user_counts[user_counts >= 10].index\n",
        "user_behavior_list = user_behavior_list[user_behavior_list['user_id'].isin(active_users)]\n",
        "\n",
        "print(f\"Active users (‚â•10 interactions): {len(active_users)}\")\n",
        "\n",
        "# Sample users for faster evaluation (optional)\n",
        "if len(active_users) > 5000:\n",
        "    rng = np.random.default_rng(seed=42)\n",
        "    sampled_users = rng.choice(active_users, size=5000, replace=False)\n",
        "    user_behavior_list = user_behavior_list[user_behavior_list['user_id'].isin(sampled_users)]\n",
        "    print(f\"Sampled users for evaluation: {len(sampled_users)}\")\n",
        "\n",
        "# Shuffle the data\n",
        "user_behavior_list = shuffle(user_behavior_list, random_state=42)\n",
        "\n",
        "# Train-test split (80-20)\n",
        "train_df = user_behavior_list.groupby('user_id', group_keys=False).apply(\n",
        "    lambda x: x.sample(frac=0.8, random_state=42)\n",
        ")\n",
        "test_df = user_behavior_list.drop(train_df.index)\n",
        "\n",
        "print(f\"Train interactions: {len(train_df)}\")\n",
        "print(f\"Test interactions: {len(test_df)}\")\n",
        "print(f\"Unique users in train: {train_df['user_id'].nunique()}\")\n",
        "print(f\"Unique users in test: {test_df['user_id'].nunique()}\")\n",
        "print(f\"Unique items in train: {train_df['track_id'].nunique()}\")\n",
        "print(f\"Unique items in test: {test_df['track_id'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Popularity Baseline Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéµ Calculating track popularity...\n",
            "Total unique tracks: 15247\n",
            "Tracks with playcount > 0: 15247\n",
            "\n",
            "Top 10 Most Popular Tracks:\n",
            "                 track_id  total_playcount\n",
            "8632   TRONYHY128F92C9D11             2589\n",
            "399    TRAOIAH128F92F707B              923\n",
            "11855  TRUFTBY128F93450B8              787\n",
            "11471  TRTNFRQ12903CB6360              757\n",
            "14994  TRZNAHL128F9327D5A              701\n",
            "21     TRAALAH128E078234A              674\n",
            "14077  TRXWAZC128F9314B3E              673\n",
            "3747   TRGCHLH12903CB7352              600\n",
            "9102   TRPFYYL128F92F7144              599\n",
            "8586   TROMKCG128F9320C09              598\n",
            "\n",
            "Item mapping created: 15247 items\n",
            "User mapping created: 5000 users\n"
          ]
        }
      ],
      "source": [
        "# Calculate track popularity from training data\n",
        "print(\"üéµ Calculating track popularity...\")\n",
        "\n",
        "track_popularity = train_df.groupby('track_id')['playcount'].sum().reset_index()\n",
        "track_popularity.rename(columns={'playcount': 'total_playcount'}, inplace=True)\n",
        "\n",
        "# Sort by popularity (descending)\n",
        "popularity_sorted = track_popularity.sort_values('total_playcount', ascending=False)\n",
        "\n",
        "print(f\"Total unique tracks: {len(track_popularity)}\")\n",
        "print(f\"Tracks with playcount > 0: {(track_popularity['total_playcount'] > 0).sum()}\")\n",
        "\n",
        "# Display top 10 most popular tracks\n",
        "print(\"\\nTop 10 Most Popular Tracks:\")\n",
        "print(popularity_sorted.head(10))\n",
        "\n",
        "# Create item mappings\n",
        "all_items = sorted(train_df['track_id'].unique())\n",
        "all_users = sorted(train_df['user_id'].unique())\n",
        "\n",
        "item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
        "user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
        "\n",
        "print(f\"\\nItem mapping created: {len(item_to_idx)} items\")\n",
        "print(f\"User mapping created: {len(user_to_idx)} users\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Generated popularity recommendations for k=5\n",
            "‚úÖ Generated popularity recommendations for k=10\n",
            "‚úÖ Generated popularity recommendations for k=15\n",
            "‚úÖ Generated popularity recommendations for k=20\n",
            "\n",
            "Recommendation summary:\n",
            "  k=5: 5000 users, 5 items per user\n",
            "  k=10: 5000 users, 10 items per user\n",
            "  k=15: 5000 users, 15 items per user\n",
            "  k=20: 5000 users, 20 items per user\n"
          ]
        }
      ],
      "source": [
        "# Create popularity-based recommendations\n",
        "def create_popularity_recommendations(user_to_idx, item_to_idx, popularity_sorted, k=20):\n",
        "    \"\"\"\n",
        "    Create popularity-based recommendations for all users.\n",
        "    \n",
        "    Args:\n",
        "        user_to_idx: User ID to index mapping\n",
        "        item_to_idx: Item ID to index mapping\n",
        "        popularity_sorted: DataFrame sorted by popularity\n",
        "        k: Number of recommendations per user\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping user indices to recommendation arrays\n",
        "    \"\"\"\n",
        "    # Get top-k most popular items\n",
        "    top_k_items = popularity_sorted.head(k)['track_id'].tolist()\n",
        "    \n",
        "    # Convert to indices\n",
        "    top_k_indices = [item_to_idx[item] for item in top_k_items if item in item_to_idx]\n",
        "    \n",
        "    # Create recommendations for all users (same recommendations for everyone)\n",
        "    recommendations = {}\n",
        "    for user_idx in user_to_idx.values():\n",
        "        recommendations[user_idx] = np.array(top_k_indices)\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "# Generate recommendations for different k values\n",
        "k_values = [5, 10, 15, 20]\n",
        "all_recommendations = {}\n",
        "\n",
        "for k in k_values:\n",
        "    recommendations = create_popularity_recommendations(user_to_idx, item_to_idx, popularity_sorted, k)\n",
        "    all_recommendations[k] = recommendations\n",
        "    print(f\"‚úÖ Generated popularity recommendations for k={k}\")\n",
        "\n",
        "print(f\"\\nRecommendation summary:\")\n",
        "for k, recs in all_recommendations.items():\n",
        "    print(f\"  k={k}: {len(recs)} users, {len(recs[list(recs.keys())[0]])} items per user\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Content Features Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic content features for evaluation\n",
        "# In a real scenario, you would use actual audio features, metadata, etc.\n",
        "print(\"üéº Creating content features...\")\n",
        "\n",
        "n_items = len(item_to_idx)\n",
        "n_features = 50  # Number of content features\n",
        "\n",
        "# Create synthetic content features (replace with real features)\n",
        "np.random.seed(42)\n",
        "item_content = np.random.randn(n_items, n_features).astype(np.float32)\n",
        "\n",
        "# L2 normalize for cosine similarity calculations\n",
        "norms = np.linalg.norm(item_content, axis=1, keepdims=True) + 1e-12\n",
        "item_content = item_content / norms\n",
        "\n",
        "print(f\"Content features created: {item_content.shape}\")\n",
        "print(f\"Features are L2-normalized: {np.allclose(np.linalg.norm(item_content, axis=1), 1.0)}\")\n",
        "\n",
        "# Display feature statistics\n",
        "print(f\"\\nFeature statistics:\")\n",
        "print(f\"  Mean: {item_content.mean():.4f}\")\n",
        "print(f\"  Std: {item_content.std():.4f}\")\n",
        "print(f\"  Min: {item_content.min():.4f}\")\n",
        "print(f\"  Max: {item_content.max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Individual Metric Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate individual metrics for each k value\n",
        "print(\"üîç Evaluating individual metrics...\")\n",
        "\n",
        "# Store results for each k value\n",
        "individual_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n--- Evaluating k={k} ---\")\n",
        "    recommendations = all_recommendations[k]\n",
        "    \n",
        "    # Calculate each metric individually\n",
        "    ndcg_score = ndcg_at_k(recommendations, test_df, user_to_idx, item_to_idx, k)\n",
        "    novelty_score = novelty_at_k(recommendations, train_df, item_to_idx, k)\n",
        "    diversity_score = diversity_ild_at_k(recommendations, item_content, k)\n",
        "    serendipity_score = serendipity_at_k(recommendations, train_df, test_df, user_to_idx, item_to_idx, item_content, k)\n",
        "    catalog_coverage = catalog_coverage_at_k(recommendations, len(item_to_idx), k)\n",
        "    user_coverage = user_coverage_at_k(recommendations, len(user_to_idx))\n",
        "    \n",
        "    # Store results\n",
        "    individual_results[k] = {\n",
        "        'ndcg': ndcg_score,\n",
        "        'novelty': novelty_score,\n",
        "        'diversity': diversity_score,\n",
        "        'serendipity': serendipity_score,\n",
        "        'catalog_coverage': catalog_coverage,\n",
        "        'user_coverage': user_coverage\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"  NDCG@{k}: {ndcg_score:.4f}\")\n",
        "    print(f\"  Novelty@{k}: {novelty_score:.4f}\")\n",
        "    print(f\"  Diversity@{k}: {diversity_score:.4f}\")\n",
        "    print(f\"  Serendipity@{k}: {serendipity_score:.4f}\")\n",
        "    print(f\"  Catalog Coverage@{k}: {catalog_coverage:.2f}%\")\n",
        "    print(f\"  User Coverage: {user_coverage:.2f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ Individual metric evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the comprehensive evaluation function\n",
        "print(\"üìä Running comprehensive evaluation...\")\n",
        "\n",
        "# Use k=20 recommendations for comprehensive evaluation\n",
        "comprehensive_results = evaluate_recommendations(\n",
        "    recommendations=all_recommendations[20],  # Use k=20 for comprehensive evaluation\n",
        "    train_df=train_df,\n",
        "    test_df=test_df,\n",
        "    user_to_idx=user_to_idx,\n",
        "    item_to_idx=item_to_idx,\n",
        "    item_content=item_content,\n",
        "    k_values=k_values,\n",
        "    metrics=['ndcg', 'novelty', 'diversity', 'serendipity', 'catalog_coverage', 'user_coverage']\n",
        ")\n",
        "\n",
        "# Print formatted results\n",
        "print_evaluation_summary(comprehensive_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations of the results\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Popularity Baseline Evaluation Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Extract data for plotting\n",
        "metrics = ['ndcg', 'novelty', 'diversity', 'serendipity', 'catalog_coverage', 'user_coverage']\n",
        "metric_titles = ['NDCG@k', 'Novelty@k', 'Diversity@k', 'Serendipity@k', 'Catalog Coverage@k (%)', 'User Coverage (%)']\n",
        "\n",
        "for i, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Get scores for this metric\n",
        "    scores = [individual_results[k][metric] for k in k_values]\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(k_values, scores, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel('k')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xticks(k_values)\n",
        "    \n",
        "    # Add value labels on points\n",
        "    for x, y in zip(k_values, scores):\n",
        "        ax.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a summary table\n",
        "print(\"\\nüìã Summary Table:\")\n",
        "summary_df = pd.DataFrame(individual_results).T\n",
        "summary_df.index.name = 'k'\n",
        "print(summary_df.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analysis and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the results and provide insights\n",
        "print(\"üîç Analysis and Insights:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# NDCG Analysis\n",
        "print(\"\\n1. NDCG@k (Ranking Quality):\")\n",
        "ndcg_scores = [individual_results[k]['ndcg'] for k in k_values]\n",
        "print(f\"   Range: {min(ndcg_scores):.4f} - {max(ndcg_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if ndcg_scores[-1] > ndcg_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures how well the ranking matches user preferences.\")\n",
        "\n",
        "# Novelty Analysis\n",
        "print(\"\\n2. Novelty@k (Item Unpopularity):\")\n",
        "novelty_scores = [individual_results[k]['novelty'] for k in k_values]\n",
        "print(f\"   Range: {min(novelty_scores):.4f} - {max(novelty_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if novelty_scores[-1] > novelty_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures how 'unpopular' recommended items are.\")\n",
        "\n",
        "# Diversity Analysis\n",
        "print(\"\\n3. Diversity@k (List Variety):\")\n",
        "diversity_scores = [individual_results[k]['diversity'] for k in k_values]\n",
        "print(f\"   Range: {min(diversity_scores):.4f} - {max(diversity_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if diversity_scores[-1] > diversity_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures variety within recommendation lists.\")\n",
        "\n",
        "# Serendipity Analysis\n",
        "print(\"\\n4. Serendipity@k (Surprising Relevance):\")\n",
        "serendipity_scores = [individual_results[k]['serendipity'] for k in k_values]\n",
        "print(f\"   Range: {min(serendipity_scores):.4f} - {max(serendipity_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if serendipity_scores[-1] > serendipity_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures surprising but relevant recommendations.\")\n",
        "\n",
        "# Coverage Analysis\n",
        "print(\"\\n5. Coverage Analysis:\")\n",
        "catalog_coverage_scores = [individual_results[k]['catalog_coverage'] for k in k_values]\n",
        "user_coverage = individual_results[k_values[0]]['user_coverage']\n",
        "print(f\"   Catalog Coverage Range: {min(catalog_coverage_scores):.2f}% - {max(catalog_coverage_scores):.2f}%\")\n",
        "print(f\"   User Coverage: {user_coverage:.2f}%\")\n",
        "print(\"   Interpretation: Higher is better. Measures how much of the catalog/users the system can recommend to.\")\n",
        "\n",
        "# Overall Assessment\n",
        "print(\"\\n6. Overall Assessment:\")\n",
        "print(\"   Popularity baseline characteristics:\")\n",
        "print(\"   ‚úÖ High user coverage (100%) - all users get recommendations\")\n",
        "print(\"   ‚úÖ Consistent recommendations across users\")\n",
        "print(\"   ‚ùå Low novelty - only recommends popular items\")\n",
        "print(\"   ‚ùå Low diversity - same items for all users\")\n",
        "print(\"   ‚ùå Low serendipity - no personalization\")\n",
        "print(\"   ‚ùå Limited catalog coverage - only top-k popular items\")\n",
        "\n",
        "print(\"\\n7. Recommendations for Improvement:\")\n",
        "print(\"   - Implement collaborative filtering for personalization\")\n",
        "print(\"   - Add content-based filtering for diversity\")\n",
        "print(\"   - Use hybrid approaches to balance popularity and personalization\")\n",
        "print(\"   - Consider user history for serendipity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "music-recommender",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
