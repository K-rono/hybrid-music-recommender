{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fa7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-Based Collaborative Filtering Music Recommendation System\n",
    "\n",
    "#This notebook implements an item-based collaborative filtering system for music recommendations. The approach focuses on finding similarities between items (songs) based on user interaction patterns, and then recommends songs similar to those a user has already interacted with.\n",
    "\n",
    "## Overview of the Approach\n",
    "# - **Data**: Music metadata and user listening behavior\n",
    "# - **Method**: Item-based collaborative filtering\n",
    "# - **Focus**: User experience metrics including diversity, novelty\n",
    "# - **Evaluation**: Ranking-sensitive metrics (NDCG@k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f64a4",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "First, we'll load the datasets and examine their structure to understand what we're working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fe6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9912fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: LOAD DATA ===\n",
      "Loading datasets...\n",
      "Music dataset shape: (50683, 21)\n",
      "User behavior dataset shape: (9711301, 3)\n",
      "\n",
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Data (Load CSVs)\n",
    "print(\"=== STEP 1: LOAD DATA ===\")\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load music metadata\n",
    "music_df = pd.read_csv('music_list.csv')\n",
    "print(f\"Music dataset shape: {music_df.shape}\")\n",
    "\n",
    "# Load user behavior data  \n",
    "behavior_df = pd.read_csv('user_behavior_list.csv')\n",
    "print(f\"User behavior dataset shape: {behavior_df.shape}\")\n",
    "\n",
    "print(\"\\nDatasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33233b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MUSIC DATASET STRUCTURE ===\n",
      "Columns: ['track_id', 'name', 'artist', 'spotify_preview_url', 'spotify_id', 'tags', 'genre', 'year', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
      "\n",
      "First few rows:\n",
      "             track_id             name           artist  \\\n",
      "0  TRIOREW128F424EAF0   Mr. Brightside      The Killers   \n",
      "1  TRRIVDJ128F429B0E8       Wonderwall            Oasis   \n",
      "2  TROUVHL128F426C441  Come as You Are          Nirvana   \n",
      "3  TRUEIND128F93038C4      Take Me Out  Franz Ferdinand   \n",
      "4  TRLNZBD128F935E4D8            Creep        Radiohead   \n",
      "\n",
      "                                 spotify_preview_url              spotify_id  \\\n",
      "0  https://p.scdn.co/mp3-preview/4d26180e6961fd46...  09ZQ5TmUG8TSL56n0knqrj   \n",
      "1  https://p.scdn.co/mp3-preview/d012e536916c927b...  06UfBBDISthj1ZJAtX4xjj   \n",
      "2  https://p.scdn.co/mp3-preview/a1c11bb1cb231031...  0keNu0t0tqsWtExGM3nT1D   \n",
      "3  https://p.scdn.co/mp3-preview/399c401370438be4...  0ancVQ9wEcHVd0RrGICTE4   \n",
      "4  https://p.scdn.co/mp3-preview/e7eb60e9466bc3a2...  01QoK9DA7VTeTSE3MNzp4I   \n",
      "\n",
      "                                                tags genre  year  duration_ms  \\\n",
      "0  rock, alternative, indie, alternative_rock, in...   NaN  2004       222200   \n",
      "1  rock, alternative, indie, pop, alternative_roc...   NaN  2006       258613   \n",
      "2   rock, alternative, alternative_rock, 90s, grunge   RnB  1991       218920   \n",
      "3  rock, alternative, indie, alternative_rock, in...   NaN  2004       237026   \n",
      "4  rock, alternative, indie, alternative_rock, in...   RnB  2008       238640   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.355   0.918    1    -4.360     1       0.0746      0.001190   \n",
      "1         0.409   0.892    2    -4.373     1       0.0336      0.000807   \n",
      "2         0.508   0.826    4    -5.783     0       0.0400      0.000175   \n",
      "3         0.279   0.664    9    -8.851     1       0.0371      0.000389   \n",
      "4         0.515   0.430    7    -9.935     1       0.0369      0.010200   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  time_signature  \n",
      "0          0.000000    0.0971    0.240  148.114               4  \n",
      "1          0.000000    0.2070    0.651  174.426               4  \n",
      "2          0.000459    0.0878    0.543  120.012               4  \n",
      "3          0.000655    0.1330    0.490  104.560               4  \n",
      "4          0.000141    0.1290    0.104   91.841               4  \n",
      "\n",
      "Dataset info:\n",
      "- Number of tracks: 50683\n",
      "- Number of unique artists: 8317\n",
      "- Missing values per column:\n",
      "track_id                   0\n",
      "name                       0\n",
      "artist                     0\n",
      "spotify_preview_url        0\n",
      "spotify_id                 0\n",
      "tags                    1127\n",
      "genre                  28335\n",
      "year                       0\n",
      "duration_ms                0\n",
      "danceability               0\n",
      "energy                     0\n",
      "key                        0\n",
      "loudness                   0\n",
      "mode                       0\n",
      "speechiness                0\n",
      "acousticness               0\n",
      "instrumentalness           0\n",
      "liveness                   0\n",
      "valence                    0\n",
      "tempo                      0\n",
      "time_signature             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Examine the structure of the datasets\n",
    "print(\"=== MUSIC DATASET STRUCTURE ===\")\n",
    "print(\"Columns:\", music_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(music_df.head())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"- Number of tracks: {len(music_df)}\")\n",
    "print(f\"- Number of unique artists: {music_df['artist'].nunique()}\")\n",
    "print(f\"- Missing values per column:\")\n",
    "print(music_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e65f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER BEHAVIOR DATASET STRUCTURE ===\n",
      "Columns: ['track_id', 'user_id', 'playcount']\n",
      "\n",
      "First few rows:\n",
      "             track_id                                   user_id  playcount\n",
      "0  TRIRLYL128F42539D1  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "1  TRFUPBA128F934F7E1  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "2  TRLQPQJ128F42AA94F  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "3  TRTUCUY128F92E1D24  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "4  TRHDDQG12903CB53EE  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "\n",
      "Dataset info:\n",
      "- Number of interactions: 9711301\n",
      "- Number of unique users: 962037\n",
      "- Number of unique tracks: 30459\n",
      "- Playcount statistics:\n",
      "count    9.711301e+06\n",
      "mean     2.630946e+00\n",
      "std      5.706324e+00\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      2.000000e+00\n",
      "max      2.948000e+03\n",
      "Name: playcount, dtype: float64\n",
      "- Missing values per column:\n",
      "track_id     0\n",
      "user_id      0\n",
      "playcount    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=== USER BEHAVIOR DATASET STRUCTURE ===\")\n",
    "print(\"Columns:\", behavior_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(behavior_df.head())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"- Number of interactions: {len(behavior_df)}\")\n",
    "print(f\"- Number of unique users: {behavior_df['user_id'].nunique()}\")\n",
    "print(f\"- Number of unique tracks: {behavior_df['track_id'].nunique()}\")\n",
    "print(f\"- Playcount statistics:\")\n",
    "print(behavior_df['playcount'].describe())\n",
    "print(f\"- Missing values per column:\")\n",
    "print(behavior_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c138c54",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We'll clean the data and perform exploratory analysis to understand user-item interaction patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53bbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 2: GROUP BY EACH USER ===\n",
      "Grouping songs of each user...\n",
      "Number of users with listening history: 962037\n",
      "Sample user-song list (first 2 users):\n",
      "  User 1: 00000b722001882066dff9d2da8a775658053ea0\n",
      "    Songs: 1 tracks\n",
      "    Sample: [('TRQEBOU128F425D087', 1)]...\n",
      "\n",
      "  User 2: 00001638d6189236866af9bbf309ae6c2347ffdc\n",
      "    Songs: 1 tracks\n",
      "    Sample: [('TRBCDMC128F1452976', 1)]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Group by each user (Group User Behavior)\n",
    "print(\"\\n=== STEP 2: GROUP BY EACH USER ===\")\n",
    "print(\"Grouping songs of each user...\")\n",
    "\n",
    "# Group by user_id and create user-song lists with track_id and playcount pairs\n",
    "user_song_list = (\n",
    "    behavior_df\n",
    "    .groupby('user_id', observed=True)[['track_id', 'playcount']]\n",
    "    .apply(lambda x: list(zip(x['track_id'], x['playcount'])))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Number of users with listening history: {len(user_song_list)}\")\n",
    "print(f\"Sample user-song list (first 2 users):\")\n",
    "for i, (user_id, songs) in enumerate(list(user_song_list.items())[:2]):\n",
    "    print(f\"  User {i+1}: {user_id}\")\n",
    "    print(f\"    Songs: {len(songs)} tracks\")\n",
    "    print(f\"    Sample: {songs[:3]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bec9449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 3: FILTER LISTENING HISTORY < 50 ===\n",
      "Removing users with less than 50 listening records...\n",
      "Number of users after filtering (>= 50 songs): 23795\n",
      "Updated behavior_df shape: (1808122, 3)\n",
      "\n",
      "Sample of filtered user-song list (first 2 users):\n",
      "  User 1: 0003a64f7a69e5b87a80b09c3772227185c235c7\n",
      "    Songs: 64 tracks\n",
      "    Sample: [('TRRCWXH128F42B917C', 1), ('TRMHZLT12903CEA646', 1), ('TROVIQH128F42B91A1', 1)]...\n",
      "\n",
      "  User 2: 00043d7bc800ceff4a90459e189eba5d442a1d3d\n",
      "    Songs: 66 tracks\n",
      "    Sample: [('TRLBHAD128F93519FC', 1), ('TRKEKLH128F428ABD4', 1), ('TRNNGMK128F423F034', 2)]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Filter listening history < 50 (Filter Listening History)\n",
    "print(\"\\n=== STEP 3: FILTER LISTENING HISTORY < 50 ===\")\n",
    "print(\"Removing users with less than 50 listening records...\")\n",
    "\n",
    "# Filter users with at least 50 songs in their listening history\n",
    "user_song_list = {user: songs for user, songs in user_song_list.items() if len(songs) >= 50}\n",
    "\n",
    "print(f\"Number of users after filtering (>= 50 songs): {len(user_song_list)}\")\n",
    "\n",
    "# Update behavior_df to include only filtered users\n",
    "behavior_df = behavior_df[behavior_df['user_id'].isin(user_song_list.keys())]\n",
    "print(f\"Updated behavior_df shape: {behavior_df.shape}\")\n",
    "\n",
    "# Display sample of filtered data\n",
    "print(f\"\\nSample of filtered user-song list (first 2 users):\")\n",
    "for i, (user_id, songs) in enumerate(list(user_song_list.items())[:2]):\n",
    "    print(f\"  User {i+1}: {user_id}\")\n",
    "    print(f\"    Songs: {len(songs)} tracks\")\n",
    "    print(f\"    Sample: {songs[:3]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1490a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 4: SAMPLE USERS (n = 5000) ===\n",
      "Sampling 5000 users randomly with seed=42...\n",
      "Total active users available: 23795\n",
      "Sampled users: 5000\n",
      "Updated behavior_df shape: (377381, 3)\n",
      "Updated user_song_list size: 5000\n",
      "\n",
      "Sample of sampled user-song list (first 2 users):\n",
      "  User 1: 0030f00cd1d9ccbff086e4ee6541a599484df3b0\n",
      "    Songs: 59 tracks\n",
      "    Sample: [('TRLBHAD128F93519FC', 1), ('TRMFANB128F9356836', 1), ('TRWZJEM128F93501BF', 1)]...\n",
      "\n",
      "  User 2: 00441d21d173bb83e7eae898313e377655ba91b6\n",
      "    Songs: 59 tracks\n",
      "    Sample: [('TRUWDZO128F9339024', 1), ('TRNYCAH12903CB19F2', 4), ('TREAQSX128E07818CA', 1)]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Sample users (n = 5000) (User Sampling)\n",
    "print(\"\\n=== STEP 4: SAMPLE USERS (n = 5000) ===\")\n",
    "print(\"Sampling 5000 users randomly with seed=42...\")\n",
    "\n",
    "# Get list of active users (users with >= 50 songs)\n",
    "active_users = list(user_song_list.keys())\n",
    "print(f\"Total active users available: {len(active_users)}\")\n",
    "\n",
    "# Sample 5000 users randomly with seed=42\n",
    "rng = np.random.default_rng(seed=42)\n",
    "sampled_users = rng.choice(active_users, size=5000, replace=False)\n",
    "\n",
    "print(f\"Sampled users: {len(sampled_users)}\")\n",
    "\n",
    "# Update user_song_list to include only sampled users\n",
    "user_song_list = {user: songs for user, songs in user_song_list.items() if user in sampled_users}\n",
    "\n",
    "# Filter the behavior_df DataFrame to include only sampled users\n",
    "behavior_df = behavior_df[behavior_df['user_id'].isin(sampled_users)]\n",
    "\n",
    "print(f\"Updated behavior_df shape: {behavior_df.shape}\")\n",
    "print(f\"Updated user_song_list size: {len(user_song_list)}\")\n",
    "\n",
    "# Display sample of sampled data\n",
    "print(f\"\\nSample of sampled user-song list (first 2 users):\")\n",
    "for i, (user_id, songs) in enumerate(list(user_song_list.items())[:2]):\n",
    "    print(f\"  User {i+1}: {user_id}\")\n",
    "    print(f\"    Songs: {len(songs)} tracks\")\n",
    "    print(f\"    Sample: {songs[:3]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267d4fe",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering / User-Item Matrix Creation\n",
    "\n",
    "Create the user-item interaction matrix that will serve as the foundation for calculating item-to-item similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81cdd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and utility functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Configuration and utility functions\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the recommendation system\"\"\"\n",
    "    MEMORY_THRESHOLD_GB = 4.0\n",
    "    TOP_K_NEIGHBORS = 10\n",
    "    BATCH_SIZE = 4096\n",
    "    DIVERSITY_WEIGHT = 0.1\n",
    "    NOVELTY_WEIGHT = 0.1\n",
    "\n",
    "def calculate_memory_usage(matrix_shape):\n",
    "    \"\"\"Calculate memory usage in GB for a matrix\"\"\"\n",
    "    return (matrix_shape[0] * matrix_shape[1] * 8) / (1024**3)\n",
    "\n",
    "def create_user_item_matrix(behavior_df, user_to_idx, item_to_idx, idx_to_user, idx_to_item):\n",
    "    \"\"\"Create user-item matrix with memory optimization\"\"\"\n",
    "    try:\n",
    "        print(\"Building sparse user-item matrix...\")\n",
    "        \n",
    "        # Map user_id and track_id to indices\n",
    "        behavior_df['user_idx'] = behavior_df['user_id'].map(user_to_idx)\n",
    "        behavior_df['item_idx'] = behavior_df['track_id'].map(item_to_idx)\n",
    "        \n",
    "        # Check for unmapped entries\n",
    "        unmapped_users = behavior_df['user_idx'].isna().sum()\n",
    "        unmapped_items = behavior_df['item_idx'].isna().sum()\n",
    "        if unmapped_users > 0 or unmapped_items > 0:\n",
    "            print(f\"Warning: {unmapped_users} users and {unmapped_items} items could not be mapped\")\n",
    "            behavior_df = behavior_df.dropna(subset=['user_idx', 'item_idx'])\n",
    "        \n",
    "        # Create sparse matrix using scipy\n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        # Extract coordinates and values\n",
    "        rows = behavior_df['user_idx'].astype(int).values\n",
    "        cols = behavior_df['item_idx'].astype(int).values  \n",
    "        data = behavior_df['playcount'].values\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        user_item_sparse = csr_matrix((data, (rows, cols)), \n",
    "                                     shape=(len(user_to_idx), len(item_to_idx)))\n",
    "        \n",
    "        print(f\"Sparse matrix shape: {user_item_sparse.shape}\")\n",
    "        print(f\"Matrix density: {user_item_sparse.nnz / (user_item_sparse.shape[0] * user_item_sparse.shape[1]):.6f}\")\n",
    "        print(f\"Non-zero elements: {user_item_sparse.nnz:,}\")\n",
    "        \n",
    "        # Check memory usage\n",
    "        memory_gb = calculate_memory_usage(user_item_sparse.shape)\n",
    "        print(f\"Dense matrix would need: {memory_gb:.2f} GB\")\n",
    "\n",
    "        # VVV FIX STARTS HERE VVV\n",
    "        # The entire if/else block was removed and replaced with this:\n",
    "        print(\"Always working with sparse matrix to ensure performance.\")\n",
    "        return user_item_sparse, user_item_sparse # Return the sparse matrix for both variables\n",
    "        # ^^^ FIX ENDS HERE ^^^\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating user-item matrix: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Configuration and utility functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f79fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: FILTER BEHAVIOR BY SAMPLED USERS ===\n",
      "Filtering behavior data to include only sampled users...\n",
      "Final behavior dataset shape: (377381, 3)\n",
      "Unique users in behavior data: 5000\n",
      "Unique tracks in behavior data: 21087\n",
      "\n",
      "=== STEP 6: 80/20 TRAIN-TEST SPLIT ===\n",
      "Performing train-test split with seed=42...\n",
      "Train set shape: (301961, 3)\n",
      "Test set shape: (75420, 3)\n",
      "Train users: 5000\n",
      "Test users: 5000\n",
      "Users in both train and test sets: 5000\n",
      "\n",
      "Summary statistics:\n",
      "  Total interactions: 377,381\n",
      "  Train interactions: 301,961 (80.0%)\n",
      "  Test interactions: 75,420 (20.0%)\n",
      "  Unique users: 5,000\n",
      "  Unique tracks: 21,087\n",
      "\n",
      "Final behavior dataset ready for model building: (377381, 3)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Filter behavior by sampled users (Filter Behavior by Sampled Users)\n",
    "print(\"\\n=== STEP 5: FILTER BEHAVIOR BY SAMPLED USERS ===\")\n",
    "print(\"Filtering behavior data to include only sampled users...\")\n",
    "\n",
    "# The behavior_df is already filtered to include only sampled users\n",
    "# This step is already completed in Step 4, but let's verify\n",
    "print(f\"Final behavior dataset shape: {behavior_df.shape}\")\n",
    "print(f\"Unique users in behavior data: {behavior_df['user_id'].nunique()}\")\n",
    "print(f\"Unique tracks in behavior data: {behavior_df['track_id'].nunique()}\")\n",
    "\n",
    "# Step 6: 80/20 Train-Test Split (Train-Test Split)\n",
    "print(\"\\n=== STEP 6: 80/20 TRAIN-TEST SPLIT ===\")\n",
    "print(\"Performing train-test split with seed=42...\")\n",
    "\n",
    "# Perform train-test split on a per-user basis (80% train, 20% test)\n",
    "train_df = behavior_df.groupby('user_id', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.8, random_state=42)\n",
    ")\n",
    "test_df = behavior_df.drop(train_df.index)\n",
    "\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Train users: {train_df['user_id'].nunique()}\")\n",
    "print(f\"Test users: {test_df['user_id'].nunique()}\")\n",
    "\n",
    "# Verify that all users in test set are also in train set\n",
    "train_users = set(train_df['user_id'].unique())\n",
    "test_users = set(test_df['user_id'].unique())\n",
    "common_users = train_users.intersection(test_users)\n",
    "print(f\"Users in both train and test sets: {len(common_users)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(f\"  Total interactions: {len(behavior_df):,}\")\n",
    "print(f\"  Train interactions: {len(train_df):,} ({len(train_df)/len(behavior_df)*100:.1f}%)\")\n",
    "print(f\"  Test interactions: {len(test_df):,} ({len(test_df)/len(behavior_df)*100:.1f}%)\")\n",
    "print(f\"  Unique users: {behavior_df['user_id'].nunique():,}\")\n",
    "print(f\"  Unique tracks: {behavior_df['track_id'].nunique():,}\")\n",
    "\n",
    "# Final behavior dataset ready for model building\n",
    "print(f\"\\nFinal behavior dataset ready for model building: {behavior_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9f6c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 7: CREATE USER-ITEM MATRIX ===\n",
      "Creating user-item interaction matrix for collaborative filtering...\n",
      "Created mappings for 5000 users and 21087 items\n",
      "Building sparse user-item matrix...\n",
      "Sparse matrix shape: (5000, 21087)\n",
      "Matrix density: 0.003579\n",
      "Non-zero elements: 377,381\n",
      "Dense matrix would need: 0.79 GB\n",
      "Always working with sparse matrix to ensure performance.\n",
      "User-item matrix ready for collaborative filtering\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create User-Item Matrix (Feature Engineering)\n",
    "print(\"\\n=== STEP 7: CREATE USER-ITEM MATRIX ===\")\n",
    "print(\"Creating user-item interaction matrix for collaborative filtering...\")\n",
    "\n",
    "# Create mappings for users and items to indices\n",
    "unique_users = behavior_df['user_id'].unique()\n",
    "unique_items = behavior_df['track_id'].unique()\n",
    "\n",
    "user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(user_to_idx)} users and {len(item_to_idx)} items\")\n",
    "\n",
    "# Use the utility function to create the matrix\n",
    "user_item_matrix, user_item_array = create_user_item_matrix(\n",
    "    behavior_df, user_to_idx, item_to_idx, idx_to_user, idx_to_item\n",
    ")\n",
    "\n",
    "print(\"User-item matrix ready for collaborative filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbed2b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 8: CREATE TRAINING MATRIX ===\n",
      "Creating training user-item matrix for model building...\n",
      "Building sparse user-item matrix...\n",
      "Sparse matrix shape: (5000, 21087)\n",
      "Matrix density: 0.002864\n",
      "Non-zero elements: 301,961\n",
      "Dense matrix would need: 0.79 GB\n",
      "Always working with sparse matrix to ensure performance.\n",
      "Training matrix ready for model building\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Create Training Matrix (Create Training Matrix)\n",
    "print(\"\\n=== STEP 8: CREATE TRAINING MATRIX ===\")\n",
    "print(\"Creating training user-item matrix for model building...\")\n",
    "\n",
    "# Use the utility function to create the training matrix\n",
    "train_matrix, train_array = create_user_item_matrix(\n",
    "    train_df, user_to_idx, item_to_idx, idx_to_user, idx_to_item\n",
    ")\n",
    "\n",
    "print(\"Training matrix ready for model building\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac868239",
   "metadata": {},
   "source": [
    "## Step 4: Item-Based Collaborative Filtering Model\n",
    "\n",
    "Implement the core item-based collaborative filtering algorithm by calculating item-to-item similarities and building the recommendation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22289c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CALCULATING ITEM SIMILARITY MATRIX (TOP-K SPARSE) ===\n",
      "Working with sparse user-item training matrix\n",
      "Item matrix shape for similarity calculation: (21087, 5000)\n",
      "Finding top-k nearest neighbors per item (cosine distance)...\n",
      "Built sparse top-50 similarity matrix: nnz=1,285,892, shape=(21087, 21087)\n",
      "Item similarity (Top-K sparse) computed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Calculate item-to-item similarity matrix (Top-K sparse, memory efficient)\n",
    "print(\"=== CALCULATING ITEM SIMILARITY MATRIX (TOP-K SPARSE) ===\")\n",
    "\n",
    "TOP_K = 50  # neighbors per item\n",
    "SIM_DTYPE = np.float32\n",
    "\n",
    "if hasattr(train_array, 'toarray'):\n",
    "    print(\"Working with sparse user-item training matrix\")\n",
    "    # Items as rows, users as columns\n",
    "    item_matrix = train_array.T.tocsr()\n",
    "    n_items, n_users = item_matrix.shape\n",
    "    print(f\"Item matrix shape for similarity calculation: ({n_items}, {n_users})\")\n",
    "\n",
    "    # Use NearestNeighbors with cosine distance on sparse matrix\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=min(TOP_K + 1, n_items), metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "    nn.fit(item_matrix)\n",
    "\n",
    "    print(\"Finding top-k nearest neighbors per item (cosine distance)...\")\n",
    "    distances, indices = nn.kneighbors(item_matrix, return_distance=True)\n",
    "\n",
    "    # Convert distances to cosine similarity: sim = 1 - dist\n",
    "    # Drop self neighbor (distance=0 at index 0)\n",
    "    distances = distances[:, 1:].astype(SIM_DTYPE)\n",
    "    indices = indices[:, 1:]\n",
    "    similarities = (1.0 - distances).clip(min=0).astype(SIM_DTYPE)\n",
    "\n",
    "    # Build sparse CSR similarity matrix\n",
    "    indptr = np.arange(0, similarities.shape[0] * similarities.shape[1] + 1, similarities.shape[1])\n",
    "    item_indices_flat = indices.ravel()\n",
    "    sims_flat = similarities.ravel()\n",
    "\n",
    "    from scipy.sparse import csr_matrix as _csr\n",
    "    item_similarity_csr = _csr((sims_flat, item_indices_flat, indptr), shape=(n_items, n_items), dtype=SIM_DTYPE)\n",
    "\n",
    "    # Symmetrize by taking max(sim(i,j), sim(j,i)) to improve recall\n",
    "    item_similarity_csr = item_similarity_csr.maximum(item_similarity_csr.T)\n",
    "\n",
    "    # Keep variable name used downstream (generic)\n",
    "    item_similarity = item_similarity_csr\n",
    "\n",
    "    print(f\"Built sparse top-{TOP_K} similarity matrix: nnz={item_similarity.nnz:,}, shape={item_similarity.shape}\")\n",
    "else:\n",
    "    print(\"Working with dense user-item training matrix\")\n",
    "    item_matrix = train_array.T\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=min(TOP_K + 1, item_matrix.shape[0]), metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "    nn.fit(item_matrix)\n",
    "\n",
    "    print(\"Finding top-k nearest neighbors per item (cosine distance, dense)...\")\n",
    "    distances, indices = nn.kneighbors(item_matrix, return_distance=True)\n",
    "\n",
    "    distances = distances[:, 1:].astype(SIM_DTYPE)\n",
    "    indices = indices[:, 1:]\n",
    "    similarities = (1.0 - distances).clip(min=0).astype(SIM_DTYPE)\n",
    "\n",
    "    # Build sparse matrix even for dense input to save memory\n",
    "    from scipy.sparse import csr_matrix as _csr\n",
    "    indptr = np.arange(0, similarities.shape[0] * similarities.shape[1] + 1, similarities.shape[1])\n",
    "    item_similarity = _csr((similarities.ravel(), indices.ravel(), indptr), shape=(item_matrix.shape[0], item_matrix.shape[0]), dtype=SIM_DTYPE)\n",
    "    item_similarity = item_similarity.maximum(item_similarity.T)\n",
    "\n",
    "    print(f\"Built sparse top-{TOP_K} similarity matrix: nnz={item_similarity.nnz:,}, shape={item_similarity.shape}\")\n",
    "\n",
    "print(\"Item similarity (Top-K sparse) computed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c250add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMILARITY ANALYSIS (TOP-K SPARSE) ===\n",
      "Similarity statistics:\n",
      "- Mean:   0.2691\n",
      "- Median: 0.1997\n",
      "- Std:    0.2122\n",
      "- Min:    0.0031\n",
      "- Max:    1.0000\n",
      "\n",
      "Top 5 most similar item pairs:\n",
      "- Chillin' - Modjo\n",
      "  <-> Greatest Story Ever Told - Grateful Dead\n",
      "  Similarity: 1.0000\n",
      "\n",
      "- Nightflight - Novaspace\n",
      "  <-> Too Shy - Kajagoogoo\n",
      "  Similarity: 1.0000\n",
      "\n",
      "- Rainbow Box - Riverside\n",
      "  <-> Vocari Dei - Pain of Salvation\n",
      "  Similarity: 1.0000\n",
      "\n",
      "- Rainbow Box - Riverside\n",
      "  <-> Back To The River - Riverside\n",
      "  Similarity: 1.0000\n",
      "\n",
      "- Beni Beni - Niyaz\n",
      "  <-> Gouge Away - Pixies\n",
      "  Similarity: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze similarity distribution (supports sparse Top-K)\n",
    "print(\"=== SIMILARITY ANALYSIS (TOP-K SPARSE) ===\")\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# item_similarity is a sparse CSR matrix produced above (or dense in rare cases)\n",
    "if 'item_similarity' in globals() and hasattr(item_similarity, 'tocsr'):\n",
    "    sim_csr = item_similarity.tocsr()\n",
    "    sim_coo = sim_csr.tocoo(copy=False)\n",
    "\n",
    "    # Exclude diagonal and keep upper triangle to avoid duplicates\n",
    "    mask = (sim_coo.row < sim_coo.col)\n",
    "    vals = sim_coo.data[mask]\n",
    "\n",
    "    if vals.size == 0:\n",
    "        print(\"No off-diagonal similarities found.\")\n",
    "    else:\n",
    "        print(\"Similarity statistics:\")\n",
    "        print(f\"- Mean:   {np.mean(vals):.4f}\")\n",
    "        print(f\"- Median: {np.median(vals):.4f}\")\n",
    "        print(f\"- Std:    {np.std(vals):.4f}\")\n",
    "        print(f\"- Min:    {np.min(vals):.4f}\")\n",
    "        print(f\"- Max:    {np.max(vals):.4f}\")\n",
    "\n",
    "        # Top-N pairs by similarity\n",
    "        n_top_pairs = 5\n",
    "        if vals.size <= n_top_pairs:\n",
    "            top_idx = np.argsort(vals)[::-1]\n",
    "        else:\n",
    "            part = np.argpartition(vals, -n_top_pairs)[-n_top_pairs:]\n",
    "            top_idx = part[np.argsort(vals[part])[::-1]]\n",
    "\n",
    "        print(f\"\\nTop {n_top_pairs} most similar item pairs:\")\n",
    "        for i in top_idx:\n",
    "            idx1 = int(sim_coo.row[mask][i])\n",
    "            idx2 = int(sim_coo.col[mask][i])\n",
    "            similarity = float(vals[i])\n",
    "\n",
    "            item1 = idx_to_item[idx1] if 'idx_to_item' in globals() else idx1\n",
    "            item2 = idx_to_item[idx2] if 'idx_to_item' in globals() else idx2\n",
    "\n",
    "            item1_name = item1\n",
    "            item2_name = item2\n",
    "            try:\n",
    "                if item1 in music_df['track_id'].values:\n",
    "                    info1 = music_df[music_df['track_id'] == item1].iloc[0]\n",
    "                    item1_name = f\"{info1['name']} - {info1['artist']}\"\n",
    "                if item2 in music_df['track_id'].values:\n",
    "                    info2 = music_df[music_df['track_id'] == item2].iloc[0]\n",
    "                    item2_name = f\"{info2['name']} - {info2['artist']}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            print(f\"- {item1_name}\")\n",
    "            print(f\"  <-> {item2_name}\")\n",
    "            print(f\"  Similarity: {similarity:.4f}\")\n",
    "            print()\n",
    "else:\n",
    "    # Dense fallback (unlikely after Top-K build)\n",
    "    sim = item_similarity if 'item_similarity' in globals() else item_similarity_matrix\n",
    "    upper_triangle = np.triu(sim, k=1)\n",
    "    similarities = upper_triangle[upper_triangle > 0]\n",
    "\n",
    "    print(f\"Similarity statistics:\")\n",
    "    print(f\"- Mean: {np.mean(similarities):.4f}\")\n",
    "    print(f\"- Median: {np.median(similarities):.4f}\")\n",
    "    print(f\"- Std: {np.std(similarities):.4f}\")\n",
    "    print(f\"- Min: {np.min(similarities):.4f}\")\n",
    "    print(f\"- Max: {np.max(similarities):.4f}\")\n",
    "\n",
    "    n_top_pairs = 5\n",
    "    sim_no_diag = sim.copy()\n",
    "    np.fill_diagonal(sim_no_diag, 0)\n",
    "    top_pairs_idx = np.unravel_index(\n",
    "        np.argpartition(sim_no_diag.ravel(), -n_top_pairs)[-n_top_pairs:],\n",
    "        sim_no_diag.shape\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTop {n_top_pairs} most similar item pairs:\")\n",
    "    for i in range(n_top_pairs):\n",
    "        idx1, idx2 = top_pairs_idx[0][i], top_pairs_idx[1][i]\n",
    "        similarity = sim_no_diag[idx1, idx2]\n",
    "        item1 = idx_to_item[idx1] if 'idx_to_item' in globals() else idx1\n",
    "        item2 = idx_to_item[idx2] if 'idx_to_item' in globals() else idx2\n",
    "        print(f\"- {item1} <-> {item2} | {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ee3ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CALCULATING ITEM POPULARITY FOR NOVELTY SCORES ===\n",
      "Popularity scores prepared for 21087 items.\n"
     ]
    }
   ],
   "source": [
    "# Calculate Item Popularity for the new novelty evaluation\n",
    "print(\"\\n=== CALCULATING ITEM POPULARITY FOR NOVELTY SCORES ===\")\n",
    "\n",
    "# Popularity is defined as the proportion of users in the training set who have listened to an item\n",
    "item_counts = np.array((train_matrix > 0).sum(axis=0)).flatten()\n",
    "total_users = train_matrix.shape[0]\n",
    "item_popularity = item_counts / total_users\n",
    "\n",
    "# Create a dictionary mapping from an item's index to its popularity score\n",
    "popularity_scores_dict = {i: pop for i, pop in enumerate(item_popularity)}\n",
    "print(f\"Popularity scores prepared for {len(popularity_scores_dict)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31e3bb",
   "metadata": {},
   "source": [
    "## Step 5: Recommendation Function\n",
    "\n",
    "Create functions to generate recommendations using the item-based collaborative filtering approach, with focus on user experience metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e88d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIALIZING ITEM-BASED RECOMMENDER ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available (CuPy). Falling back to CPU. Reason: No module named 'cupy'\n",
      "Recommender initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class ItemBasedRecommender:\n",
    "    \"\"\"\n",
    "    Item-Based Collaborative Filtering Recommender System (Memory Efficient)\n",
    "    Supports sparse Top-K item similarity and UX-oriented reranking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_item_matrix, item_similarity, music_metadata, \n",
    "                 user_to_idx=None, item_to_idx=None, idx_to_user=None, idx_to_item=None):\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.item_similarity = item_similarity  # CSR sparse or dense\n",
    "        self.music_metadata = music_metadata\n",
    "        \n",
    "        # Handle both sparse and dense user-item matrices\n",
    "        if hasattr(user_item_matrix, 'toarray'):  # Sparse matrix\n",
    "            self.is_sparse = True\n",
    "            self.user_to_idx = user_to_idx\n",
    "            self.item_to_idx = item_to_idx\n",
    "            self.idx_to_user = idx_to_user\n",
    "            self.idx_to_item = idx_to_item\n",
    "            self.users = list(user_to_idx.keys()) if user_to_idx else []\n",
    "            self.items = list(item_to_idx.keys()) if item_to_idx else []\n",
    "        else:  # Dense matrix (DataFrame)\n",
    "            self.is_sparse = False\n",
    "            self.items = list(user_item_matrix.columns)\n",
    "            self.users = list(user_item_matrix.index)\n",
    "        \n",
    "        # Create item metadata lookup\n",
    "        self.item_metadata = music_metadata.set_index('track_id').to_dict('index')\n",
    "\n",
    "        # GPU members\n",
    "        self.gpu_enabled = False\n",
    "        self.batch_size = 1024\n",
    "        self._cp = None\n",
    "        self._cupyx = None\n",
    "        self.item_similarity_gpu = None\n",
    "        self.user_item_T_gpu = None\n",
    "    \n",
    "    def enable_gpu(self, batch_size=2048):\n",
    "        \"\"\"Enable GPU acceleration using CuPy if available.\"\"\"\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            import cupyx\n",
    "            from cupyx.scipy.sparse import csr_matrix as gpu_csr\n",
    "        except Exception as e:\n",
    "            print(f\"GPU not available (CuPy). Falling back to CPU. Reason: {e}\")\n",
    "            self.gpu_enabled = False\n",
    "            return\n",
    "        if not (hasattr(self.item_similarity, 'tocsr') and self.is_sparse):\n",
    "            print(\"GPU path requires sparse user-item and sparse item similarity. Using CPU.\")\n",
    "            self.gpu_enabled = False\n",
    "            return\n",
    "        # Move matrices to GPU\n",
    "        self._cp = cp\n",
    "        self._cupyx = cupyx\n",
    "        self.batch_size = int(batch_size)\n",
    "        # item similarity (items x items)\n",
    "        self.item_similarity_gpu = gpu_csr((self.item_similarity.data.astype(np.float32),\n",
    "                                            self.item_similarity.indices,\n",
    "                                            self.item_similarity.indptr),\n",
    "                                           shape=self.item_similarity.shape)\n",
    "        # user-item transpose (items x users)\n",
    "        ui_T = self.user_item_matrix.T.tocsr()\n",
    "        self.user_item_T_gpu = gpu_csr((ui_T.data.astype(np.float32), ui_T.indices, ui_T.indptr),\n",
    "                                       shape=ui_T.shape)\n",
    "        self.gpu_enabled = True\n",
    "        print(f\"GPU enabled. Batch size={self.batch_size}\")\n",
    "    \n",
    "    def _get_similarity(self, item_a, item_b):\n",
    "        \"\"\"Return similarity between two items using sparse/dense similarity.\"\"\"\n",
    "        if item_a not in self.items or item_b not in self.items:\n",
    "            return 0.0\n",
    "        if hasattr(self.item_similarity, 'tocsr') and self.is_sparse:\n",
    "            ia = self.item_to_idx[item_a]\n",
    "            ib = self.item_to_idx[item_b]\n",
    "            row = self.item_similarity.getrow(ia)\n",
    "            cols = row.indices\n",
    "            data = row.data\n",
    "            pos = np.searchsorted(cols, ib)\n",
    "            if pos < len(cols) and cols[pos] == ib:\n",
    "                return float(data[pos])\n",
    "            return 0.0\n",
    "        else:\n",
    "            if hasattr(self.item_similarity, 'loc'):\n",
    "                return float(self.item_similarity.loc[item_a, item_b])\n",
    "            else:\n",
    "                ia = self.items.index(item_a)\n",
    "                ib = self.items.index(item_b)\n",
    "                return float(self.item_similarity[ia, ib])\n",
    "        \n",
    "    def get_user_interacted_items(self, user_id):\n",
    "        \"\"\"Get items that a user has interacted with\"\"\"\n",
    "        if self.is_sparse:\n",
    "            if user_id not in self.user_to_idx:\n",
    "                return []\n",
    "            user_idx = self.user_to_idx[user_id]\n",
    "            user_row = self.user_item_matrix[user_idx]\n",
    "            _, item_indices = user_row.nonzero()\n",
    "            return [self.idx_to_item[idx] for idx in item_indices]\n",
    "        else:\n",
    "            if user_id not in self.user_item_matrix.index:\n",
    "                return []\n",
    "            user_interactions = self.user_item_matrix.loc[user_id]\n",
    "            return user_interactions[user_interactions > 0].index.tolist()\n",
    "    \n",
    "    def predict_item_score(self, user_id, target_item, k_neighbors=50):\n",
    "        \"\"\"Predict score for a target item using item-based CF with Top-K sim.\"\"\"\n",
    "        if target_item not in self.items:\n",
    "            return 0.0\n",
    "        user_items = self.get_user_interacted_items(user_id)\n",
    "        if not user_items:\n",
    "            return 0.0\n",
    "        \n",
    "        target_similarities = []\n",
    "        user_ratings = []\n",
    "        for item in user_items:\n",
    "            sim = self._get_similarity(target_item, item)\n",
    "            if sim > 0:\n",
    "                if self.is_sparse:\n",
    "                    user_idx = self.user_to_idx[user_id]\n",
    "                    item_idx = self.item_to_idx[item]\n",
    "                    rating = self.user_item_matrix[user_idx, item_idx]\n",
    "                else:\n",
    "                    rating = self.user_item_matrix.loc[user_id, item]\n",
    "                target_similarities.append(sim)\n",
    "                user_ratings.append(float(rating))\n",
    "        if not target_similarities:\n",
    "            return 0.0\n",
    "        \n",
    "        if len(target_similarities) > k_neighbors:\n",
    "            order = np.argsort(target_similarities)[-k_neighbors:]\n",
    "            sims = np.array(target_similarities)[order]\n",
    "            ratings = np.array(user_ratings)[order]\n",
    "        else:\n",
    "            sims = np.array(target_similarities)\n",
    "            ratings = np.array(user_ratings)\n",
    "        denom = sims.sum()\n",
    "        if denom <= 0:\n",
    "            return 0.0\n",
    "        return float((sims * ratings).sum() / denom)\n",
    "    \n",
    "    def recommend_for_users_batch(self, user_ids, n_recommendations=10):\n",
    "        \"\"\"Batch recommend. Uses GPU sparse matmul if enabled; else CPU fallback.\"\"\"\n",
    "        if not user_ids:\n",
    "            return {}\n",
    "        if self.gpu_enabled:\n",
    "            cp = self._cp\n",
    "            # Map to indices that exist in training\n",
    "            valid_ids = [u for u in user_ids if u in self.user_to_idx]\n",
    "            if not valid_ids:\n",
    "                return {}\n",
    "            uidx = cp.asarray([self.user_to_idx[u] for u in valid_ids], dtype=cp.int32)\n",
    "            # Build submatrix R = UI_T[:, uidx]  (items x batch)\n",
    "            R_batch = self.user_item_T_gpu[:, uidx]\n",
    "            # Compute scores: S * R  => (items x items) @ (items x batch) = items x batch\n",
    "            scores = self.item_similarity_gpu @ R_batch\n",
    "            # Mask already interacted items: set to -inf\n",
    "            # Build mask from R_batch > 0\n",
    "            mask = R_batch.copy()\n",
    "            mask.data[:] = 1.0\n",
    "            # Convert scores to dense per column only for topk selection\n",
    "            # Use argpartition on GPU\n",
    "            results = {}\n",
    "            scores_csc = scores.tocsc()  # better column access\n",
    "            for j, uid in enumerate(valid_ids):\n",
    "                col = scores_csc.getcol(j).toarray().ravel()\n",
    "                # zero out seen items\n",
    "                seen = R_batch.getcol(j).toarray().ravel() > 0\n",
    "                col[seen] = -np.inf\n",
    "                k = min(n_recommendations * 2, len(col))\n",
    "                topk_idx = cp.asnumpy(cp.argpartition(cp.asarray(col), -k)[-k:])\n",
    "                topk_sorted = topk_idx[np.argsort(col[topk_idx])[::-1]][:n_recommendations]\n",
    "                items = [self.idx_to_item[i] for i in topk_sorted]\n",
    "                scores_vals = [float(col[i]) for i in topk_sorted]\n",
    "                results[uid] = list(zip(items, scores_vals))\n",
    "            return results\n",
    "        else:\n",
    "            # CPU sparse batch: S (csr) @ R (items x batch)\n",
    "            if not (hasattr(self.item_similarity, 'tocsr') and self.is_sparse):\n",
    "                # fallback to per-user path\n",
    "                return {u: self.get_recommendations(u, n_recommendations) for u in user_ids}\n",
    "            valid_ids = [u for u in user_ids if u in self.user_to_idx]\n",
    "            if not valid_ids:\n",
    "                return {}\n",
    "            uidx = np.array([self.user_to_idx[u] for u in valid_ids], dtype=np.int32)\n",
    "            R_batch = self.user_item_matrix.T.tocsr()[:, uidx]\n",
    "            scores = self.item_similarity @ R_batch\n",
    "            results = {}\n",
    "            from numpy import argpartition\n",
    "            scores_csc = scores.tocsc()\n",
    "            for j, uid in enumerate(valid_ids):\n",
    "                col = scores_csc.getcol(j).toarray().ravel()\n",
    "                seen = R_batch.getcol(j).toarray().ravel() > 0\n",
    "                col[seen] = -np.inf\n",
    "                k = min(n_recommendations * 2, len(col))\n",
    "                topk_idx = argpartition(col, -k)[-k:]\n",
    "                topk_sorted = topk_idx[np.argsort(col[topk_idx])[::-1]][:n_recommendations]\n",
    "                items = [self.idx_to_item[i] for i in topk_sorted]\n",
    "                scores_vals = [float(col[i]) for i in topk_sorted]\n",
    "                results[uid] = list(zip(items, scores_vals))\n",
    "            return results\n",
    "    \n",
    "    def get_recommendations(self, user_id, n_recommendations=10, \n",
    "                            diversity_weight=0.1, novelty_weight=0.1):\n",
    "        \"\"\"Generate recommendations for a user with UX-aware reranking.\"\"\"\n",
    "        if user_id not in self.users:\n",
    "            return []\n",
    "        user_items = set(self.get_user_interacted_items(user_id))\n",
    "        candidate_items = [item for item in self.items if item not in user_items]\n",
    "        if not candidate_items:\n",
    "            return []\n",
    "        \n",
    "        item_scores = []\n",
    "        for item in candidate_items:\n",
    "            score = self.predict_item_score(user_id, item)\n",
    "            if score > 0:\n",
    "                item_scores.append((item, score))\n",
    "        item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        final_recommendations = self._apply_diversity_novelty(\n",
    "            item_scores, user_items, n_recommendations, diversity_weight, novelty_weight\n",
    "        )\n",
    "        return final_recommendations[:n_recommendations]\n",
    "    \n",
    "    def _apply_diversity_novelty(self, scored_items, user_items, \n",
    "                                 n_recommendations, diversity_weight, novelty_weight):\n",
    "        if not scored_items:\n",
    "            return []\n",
    "        recommendations = []\n",
    "        considered_genres = set()\n",
    "        considered_artists = set()\n",
    "        user_genres = set()\n",
    "        user_artists = set()\n",
    "        for item in user_items:\n",
    "            if item in self.item_metadata:\n",
    "                meta = self.item_metadata[item]\n",
    "                if 'genre' in meta and meta['genre']:\n",
    "                    user_genres.add(meta['genre'])\n",
    "                if 'artist' in meta:\n",
    "                    user_artists.add(meta['artist'])\n",
    "        for item, base_score in scored_items:\n",
    "            if len(recommendations) >= n_recommendations:\n",
    "                break\n",
    "            diversity_bonus = 0\n",
    "            novelty_bonus = 0\n",
    "            if item in self.item_metadata:\n",
    "                meta = self.item_metadata[item]\n",
    "                g = meta.get('genre', '')\n",
    "                a = meta.get('artist', '')\n",
    "                if g and g not in considered_genres:\n",
    "                    diversity_bonus += diversity_weight * 0.5\n",
    "                if a and a not in considered_artists:\n",
    "                    diversity_bonus += diversity_weight * 0.5\n",
    "                if g and g not in user_genres:\n",
    "                    novelty_bonus += novelty_weight * 0.5\n",
    "                if a and a not in user_artists:\n",
    "                    novelty_bonus += novelty_weight * 0.5\n",
    "            final_score = base_score + diversity_bonus + novelty_bonus\n",
    "            recommendations.append((item, final_score, base_score))\n",
    "            if item in self.item_metadata:\n",
    "                meta = self.item_metadata[item]\n",
    "                if meta.get('genre'):\n",
    "                    considered_genres.add(meta['genre'])\n",
    "                if meta.get('artist'):\n",
    "                    considered_artists.add(meta['artist'])\n",
    "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [(item, s) for item, s, _ in recommendations]\n",
    "\n",
    "# Initialize the recommender\n",
    "print(\"=== INITIALIZING ITEM-BASED RECOMMENDER ===\")\n",
    "\n",
    "# Pass the appropriate mappings for sparse matrix support\n",
    "if hasattr(train_matrix, 'toarray'):  # If sparse matrix\n",
    "    recommender = ItemBasedRecommender(\n",
    "        train_matrix, item_similarity, music_df,\n",
    "        user_to_idx, item_to_idx, idx_to_user, idx_to_item\n",
    "    )\n",
    "else:  # Dense matrix\n",
    "    recommender = ItemBasedRecommender(train_matrix, item_similarity, music_df)\n",
    "\n",
    "# Try enable GPU for batched recommendation\n",
    "recommender.enable_gpu(batch_size=4096)\n",
    "\n",
    "print(\"Recommender initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e772f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the recommendation function\n",
    "# print(\"=== TESTING RECOMMENDATION FUNCTION ===\")\n",
    "\n",
    "# # Get a sample user for testing\n",
    "# if hasattr(train_matrix, 'toarray'):  # Sparse matrix\n",
    "#     sample_user = list(user_to_idx.keys())[0]\n",
    "# else:  # Dense matrix\n",
    "#     sample_user = train_matrix.index[0]\n",
    "\n",
    "# print(f\"Testing recommendations for user: {sample_user}\")\n",
    "\n",
    "# # Get user's interaction history\n",
    "# user_history = recommender.get_user_interacted_items(sample_user)\n",
    "# print(f\"User has interacted with {len(user_history)} items\")\n",
    "\n",
    "# # Show some of their interactions with metadata\n",
    "# print(\"\\nUser's listening history (sample):\")\n",
    "# for i, track_id in enumerate(user_history[:5]):\n",
    "#     if track_id in recommender.item_metadata:\n",
    "#         track_info = recommender.item_metadata[track_id]\n",
    "#         print(f\"  {i+1}. {track_info.get('name', 'Unknown')} - {track_info.get('artist', 'Unknown Artist')}\")\n",
    "\n",
    "# # Generate recommendations\n",
    "# print(\"Generating recommendations...\")\n",
    "# recommendations = recommender.get_recommendations(sample_user, n_recommendations=5)\n",
    "# print(f\"\\nGenerated {len(recommendations)} recommendations:\")\n",
    "\n",
    "# for i, (track_id, score) in enumerate(recommendations):\n",
    "#     if track_id in recommender.item_metadata:\n",
    "#         track_info = recommender.item_metadata[track_id]\n",
    "#         print(f\"  {i+1}. {track_info.get('name', 'Unknown')} - {track_info.get('artist', 'Unknown Artist')} (Score: {score:.4f})\")\n",
    "#     else:\n",
    "#         print(f\"  {i+1}. {track_id} (Score: {score:.4f})\")\n",
    "\n",
    "# # Test memory usage\n",
    "# import psutil\n",
    "# import os\n",
    "# process = psutil.Process(os.getpid())\n",
    "# memory_gb = process.memory_info().rss / 1024 / 1024 / 1024\n",
    "# print(f\"\\n💾 Current memory usage: {memory_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96403ed",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation\n",
    "\n",
    "Implement comprehensive evaluation metrics focusing on both ranking accuracy and user experience quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ff446ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STANDALONE EVALUATION FRAMEWORK READY ===\n",
      "Standalone evaluation metrics imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import the standalone evaluation metrics\n",
    "from standalone_evaluation_metrics import evaluate_recommendations, print_evaluation_summary\n",
    "\n",
    "print(\"=== STANDALONE EVALUATION FRAMEWORK READY ===\")\n",
    "print(\"Standalone evaluation metrics imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATING RECOMMENDATIONS FOR EVALUATION (FAST BATCH METHOD) ===\n",
      "Generating recommendations for 5000 test users...\n",
      "  Processed batch 1/10...\n",
      "  Processed batch 2/10...\n",
      "  Processed batch 3/10...\n",
      "  Processed batch 4/10...\n",
      "  Processed batch 5/10...\n",
      "  Processed batch 6/10...\n",
      "  Processed batch 7/10...\n",
      "  Processed batch 8/10...\n",
      "  Processed batch 9/10...\n",
      "  Processed batch 10/10...\n",
      "\n",
      "Generated recommendations for 5000 users.\n",
      "Converting recommendations to the required index format for evaluation...\n",
      "Successfully prepared recommendations for 5000 users for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations for evaluation using the FAST BATCH method\n",
    "print(\"\\n=== GENERATING RECOMMENDATIONS FOR EVALUATION (FAST BATCH METHOD) ===\")\n",
    "\n",
    "# Get all users that are in the test set\n",
    "test_users_list = test_df['user_id'].unique().tolist()\n",
    "print(f\"Generating recommendations for {len(test_users_list)} test users...\")\n",
    "\n",
    "# Define a batch size to process users in chunks (avoids memory issues)\n",
    "BATCH_SIZE = 512 \n",
    "recommendations = {}\n",
    "total_batches = (len(test_users_list) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in range(0, len(test_users_list), BATCH_SIZE):\n",
    "    # Get a batch of user IDs\n",
    "    batch_user_ids = test_users_list[i:i + BATCH_SIZE]\n",
    "    \n",
    "    # Use the fast batch recommendation function\n",
    "    batch_recs = recommender.recommend_for_users_batch(\n",
    "        user_ids=batch_user_ids, \n",
    "        n_recommendations=20\n",
    "    )\n",
    "    \n",
    "    # Update the main recommendations dictionary with the batch results\n",
    "    recommendations.update(batch_recs)\n",
    "    \n",
    "    # Print progress\n",
    "    current_batch_num = (i // BATCH_SIZE) + 1\n",
    "    print(f\"  Processed batch {current_batch_num}/{total_batches}...\")\n",
    "\n",
    "print(f\"\\nGenerated recommendations for {len(recommendations)} users.\")\n",
    "\n",
    "# The evaluation script expects item INDICES, not item IDs.\n",
    "# We need to convert the track_id recommendations into item_idx recommendations.\n",
    "print(\"Converting recommendations to the required index format for evaluation...\")\n",
    "final_recommendations_indexed = {}\n",
    "for user_id, rec_list in recommendations.items():\n",
    "    if user_id in user_to_idx:\n",
    "        user_idx = user_to_idx[user_id]\n",
    "        \n",
    "        # Convert track_ids to item_indices, filtering out any not in the mapping\n",
    "        rec_indices = [item_to_idx[track_id] for track_id, score in rec_list if track_id in item_to_idx]\n",
    "        \n",
    "        if rec_indices:\n",
    "            final_recommendations_indexed[user_idx] = np.array(rec_indices)\n",
    "\n",
    "print(f\"Successfully prepared recommendations for {len(final_recommendations_indexed)} users for evaluation.\")\n",
    "\n",
    "# IMPORTANT: The evaluation function below must use the NEW variable 'final_recommendations_indexed'\n",
    "# We rename it here to 'recommendations' to match what the original code expects.\n",
    "recommendations = final_recommendations_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ce226a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREPARING ITEM CONTENT FEATURES ===\n",
      "Item content features prepared with shape: (21087, 5000)\n",
      "\n",
      "=== RUNNING EVALUATION WITH STANDALONE METRICS ===\n",
      "Evaluating 5000 users across 4 k values...\n",
      "  Calculating ndcg...\n",
      "    ndcg@5: 0.2170\n",
      "    ndcg@10: 0.1747\n",
      "    ndcg@15: 0.1672\n",
      "    ndcg@20: 0.1729\n",
      "  Calculating novelty...\n",
      "    novelty@5: 7.8330\n",
      "    novelty@10: 7.9205\n",
      "    novelty@15: 7.9586\n",
      "    novelty@20: 7.9954\n",
      "  Calculating diversity...\n",
      "    diversity@5: 0.5642\n",
      "    diversity@10: 0.6494\n",
      "    diversity@15: 0.7094\n",
      "    diversity@20: 0.7529\n",
      "  Calculating serendipity...\n",
      "    serendipity@5: 0.6888\n",
      "    serendipity@10: 0.7063\n",
      "    serendipity@15: 0.7181\n",
      "    serendipity@20: 0.7273\n",
      "  Calculating catalog_coverage...\n",
      "    catalog_coverage@5: 33.0535\n",
      "    catalog_coverage@10: 48.3284\n",
      "    catalog_coverage@15: 58.3061\n",
      "    catalog_coverage@20: 65.3199\n",
      "  Calculating user_coverage...\n",
      "    user_coverage@5: 100.0000\n",
      "    user_coverage@10: 100.0000\n",
      "    user_coverage@15: 100.0000\n",
      "    user_coverage@20: 100.0000\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATION EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "NDCG:\n",
      "--------------------\n",
      "  @ 5: 0.2170\n",
      "  @10: 0.1747\n",
      "  @15: 0.1672\n",
      "  @20: 0.1729\n",
      "\n",
      "NOVELTY:\n",
      "--------------------\n",
      "  @ 5: 7.8330\n",
      "  @10: 7.9205\n",
      "  @15: 7.9586\n",
      "  @20: 7.9954\n",
      "\n",
      "DIVERSITY:\n",
      "--------------------\n",
      "  @ 5: 0.5642\n",
      "  @10: 0.6494\n",
      "  @15: 0.7094\n",
      "  @20: 0.7529\n",
      "\n",
      "SERENDIPITY:\n",
      "--------------------\n",
      "  @ 5: 0.6888\n",
      "  @10: 0.7063\n",
      "  @15: 0.7181\n",
      "  @20: 0.7273\n",
      "\n",
      "CATALOG_COVERAGE:\n",
      "--------------------\n",
      "  @ 5:  33.05%\n",
      "  @10:  48.33%\n",
      "  @15:  58.31%\n",
      "  @20:  65.32%\n",
      "\n",
      "USER_COVERAGE:\n",
      "--------------------\n",
      "  @ 5: 100.00%\n",
      "  @10: 100.00%\n",
      "  @15: 100.00%\n",
      "  @20: 100.00%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Prepare item content features for diversity and serendipity calculations\n",
    "print(\"\\n=== PREPARING ITEM CONTENT FEATURES ===\")\n",
    "\n",
    "# Use the item-user matrix (transpose of train_matrix) as the feature representation\n",
    "# Convert to dense array and L2-normalize for proper cosine similarity calculations\n",
    "item_content = train_matrix.T.toarray().astype(np.float32)\n",
    "norms = np.linalg.norm(item_content, axis=1, keepdims=True) + 1e-12\n",
    "item_content = item_content / norms\n",
    "\n",
    "print(f\"Item content features prepared with shape: {item_content.shape}\")\n",
    "\n",
    "# Run evaluation using standalone metrics\n",
    "print(\"\\n=== RUNNING EVALUATION WITH STANDALONE METRICS ===\")\n",
    "\n",
    "evaluation_results = evaluate_recommendations(\n",
    "    recommendations=recommendations, # This uses the variable from the code I provided\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    user_to_idx=user_to_idx,\n",
    "    item_to_idx=item_to_idx,\n",
    "    item_content=item_content,\n",
    "    k_values=[5, 10, 15, 20],\n",
    "    metrics=['ndcg', 'novelty', 'diversity', 'serendipity', 'catalog_coverage', 'user_coverage']\n",
    ")\n",
    "\n",
    "# Print results using the standalone metrics summary function\n",
    "print_evaluation_summary(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
