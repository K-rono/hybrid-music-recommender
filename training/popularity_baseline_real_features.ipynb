{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Popularity Baseline Evaluation with Real Features\n",
        "\n",
        "This notebook evaluates a popularity-based recommendation system using comprehensive evaluation metrics at k values of 5, 10, 15, and 20, with real music content features.\n",
        "\n",
        "## Overview\n",
        "- **Baseline**: Popularity-based recommendations (most popular items for all users)\n",
        "- **Features**: Real audio features, metadata, and embeddings from music data\n",
        "- **Evaluation Metrics**: NDCG, Novelty, Diversity, Serendipity, Coverage\n",
        "- **K Values**: 5, 10, 15, 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\music-recommender\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Evaluation metrics imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import the standalone evaluation metrics\n",
        "# Make sure standalone_evaluation_metrics.py is in the same directory\n",
        "from standalone_evaluation_metrics import (\n",
        "    quick_evaluate,\n",
        "    evaluate_recommendations,\n",
        "    print_evaluation_summary,\n",
        "    ndcg_at_k,\n",
        "    novelty_at_k,\n",
        "    diversity_ild_at_k,\n",
        "    serendipity_at_k,\n",
        "    catalog_coverage_at_k,\n",
        "    user_coverage_at_k\n",
        ")\n",
        "\n",
        "print(\"✅ Evaluation metrics imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data loaded successfully!\n",
            "Music list shape: (50683, 21)\n",
            "User behavior list shape: (9711301, 3)\n",
            "\n",
            "Music list columns: ['track_id', 'name', 'artist', 'spotify_preview_url', 'spotify_id', 'tags', 'genre', 'year', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
            "User behavior list columns: ['track_id', 'user_id', 'playcount']\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "# Update the path to your data files\n",
        "dataset_path = ''  # Update this path to your data directory\n",
        "\n",
        "try:\n",
        "    music_list = pd.read_csv(dataset_path + 'music_list.csv')\n",
        "    user_behavior_list = pd.read_csv(dataset_path + 'user_behavior_list.csv')\n",
        "    print(\"✅ Data loaded successfully!\")\n",
        "    print(f\"Music list shape: {music_list.shape}\")\n",
        "    print(f\"User behavior list shape: {user_behavior_list.shape}\")\n",
        "    \n",
        "    # Display basic info about the datasets\n",
        "    print(f\"\\nMusic list columns: {list(music_list.columns)}\")\n",
        "    print(f\"User behavior list columns: {list(user_behavior_list.columns)}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error loading data: {e}\")\n",
        "    print(\"Please update the dataset_path variable with the correct path to your data files.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Preprocessing data...\n",
            "Active users (≥10 interactions): 23795\n",
            "Sampled users for evaluation: 5000\n",
            "Train interactions: 303007\n",
            "Test interactions: 75709\n",
            "Unique users in train: 5000\n",
            "Unique users in test: 5000\n",
            "Unique items in train: 20008\n",
            "Unique items in test: 13734\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing following the popularity baseline approach\n",
        "print(\"📊 Preprocessing data...\")\n",
        "\n",
        "# Filter users with at least 50\n",
        "user_counts = user_behavior_list['user_id'].value_counts()\n",
        "active_users = user_counts[user_counts >= 50].index\n",
        "user_behavior_list = user_behavior_list[user_behavior_list['user_id'].isin(active_users)]\n",
        "\n",
        "print(f\"Active users (≥10 interactions): {len(active_users)}\")\n",
        "\n",
        "# Sample users for faster evaluation (optional)\n",
        "if len(active_users) > 5000:\n",
        "    rng = np.random.default_rng(seed=42)\n",
        "    sampled_users = rng.choice(active_users, size=5000, replace=False)\n",
        "    user_behavior_list = user_behavior_list[user_behavior_list['user_id'].isin(sampled_users)]\n",
        "    print(f\"Sampled users for evaluation: {len(sampled_users)}\")\n",
        "\n",
        "# Shuffle the data\n",
        "user_behavior_list = shuffle(user_behavior_list, random_state=42)\n",
        "\n",
        "# Train-test split (80-20)\n",
        "train_df = user_behavior_list.groupby('user_id', group_keys=False).apply(\n",
        "    lambda x: x.sample(frac=0.8, random_state=42)\n",
        ")\n",
        "test_df = user_behavior_list.drop(train_df.index)\n",
        "\n",
        "print(f\"Train interactions: {len(train_df)}\")\n",
        "print(f\"Test interactions: {len(test_df)}\")\n",
        "print(f\"Unique users in train: {train_df['user_id'].nunique()}\")\n",
        "print(f\"Unique users in test: {test_df['user_id'].nunique()}\")\n",
        "print(f\"Unique items in train: {train_df['track_id'].nunique()}\")\n",
        "print(f\"Unique items in test: {test_df['track_id'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Real Content Features Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎼 Creating real content features...\n",
            "Filtered music list shape: (20008, 21)\n",
            "Available columns in music_list: ['track_id', 'name', 'artist', 'spotify_preview_url', 'spotify_id', 'tags', 'genre', 'year', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
            "Available numeric features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'year']\n",
            "Available categorical features: ['genre', 'artist', 'tags']\n"
          ]
        }
      ],
      "source": [
        "# Create real content features from music data\n",
        "print(\"🎼 Creating real content features...\")\n",
        "\n",
        "# Get unique tracks from training data\n",
        "all_items = sorted(train_df['track_id'].unique())\n",
        "all_users = sorted(train_df['user_id'].unique())\n",
        "\n",
        "# Filter music_list to only include tracks that are in our training data\n",
        "sampled_music_list = music_list[music_list['track_id'].isin(all_items)].copy()\n",
        "print(f\"Filtered music list shape: {sampled_music_list.shape}\")\n",
        "\n",
        "# Check available columns\n",
        "print(f\"Available columns in music_list: {list(sampled_music_list.columns)}\")\n",
        "\n",
        "# Define numeric features (audio features)\n",
        "numeric_features = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "                   'instrumentalness', 'liveness', 'valence', 'tempo', 'year']\n",
        "\n",
        "# Check which numeric features are available\n",
        "available_numeric = [col for col in numeric_features if col in sampled_music_list.columns]\n",
        "print(f\"Available numeric features: {available_numeric}\")\n",
        "\n",
        "# Define categorical features\n",
        "onehot_features = ['genre'] if 'genre' in sampled_music_list.columns else []\n",
        "hash_features = ['artist', 'tags']\n",
        "\n",
        "# Check which categorical features are available\n",
        "available_categorical = [col for col in onehot_features + hash_features if col in sampled_music_list.columns]\n",
        "print(f\"Available categorical features: {available_categorical}\")\n",
        "\n",
        "# Reset index for consistent indexing\n",
        "original_track_ids = sampled_music_list['track_id']\n",
        "sampled_music_list = sampled_music_list.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing numeric features...\n",
            "Processed 10 numeric features\n",
            "Processing categorical features...\n",
            "Processing genre features...\n",
            "Processed 16 genre features\n",
            "Processing artist features...\n",
            "Processed 500 artist features\n",
            "Processing tag features...\n",
            "Created 384 tag embeddings\n"
          ]
        }
      ],
      "source": [
        "# Process numeric features\n",
        "print(\"Processing numeric features...\")\n",
        "if available_numeric:\n",
        "    numeric_data = sampled_music_list[available_numeric]\n",
        "    \n",
        "    # Handle missing values\n",
        "    numeric_data = numeric_data.fillna(numeric_data.median())\n",
        "    \n",
        "    # Scale numeric features\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_numeric_features = scaler.fit_transform(numeric_data)\n",
        "    scaled_numeric_df = pd.DataFrame(scaled_numeric_features, columns=available_numeric)\n",
        "    print(f\"Processed {len(available_numeric)} numeric features\")\n",
        "else:\n",
        "    print(\"No numeric features available, creating empty dataframe\")\n",
        "    scaled_numeric_df = pd.DataFrame(index=sampled_music_list.index)\n",
        "\n",
        "# Process categorical features\n",
        "print(\"Processing categorical features...\")\n",
        "encoded_genre_df = pd.DataFrame(index=sampled_music_list.index)\n",
        "hashed_artist_df = pd.DataFrame(index=sampled_music_list.index)\n",
        "tag_embeddings_df = pd.DataFrame(index=sampled_music_list.index)\n",
        "\n",
        "# One-hot encode genre if available\n",
        "if 'genre' in sampled_music_list.columns:\n",
        "    print(\"Processing genre features...\")\n",
        "    onehot_data = sampled_music_list[['genre']]\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    encoded_genre_features = encoder.fit_transform(onehot_data)\n",
        "    ohe_feature_names = encoder.get_feature_names_out(['genre'])\n",
        "    encoded_genre_df = pd.DataFrame(encoded_genre_features, columns=ohe_feature_names)\n",
        "    print(f\"Processed {len(ohe_feature_names)} genre features\")\n",
        "\n",
        "# Feature hash artist if available\n",
        "if 'artist' in sampled_music_list.columns:\n",
        "    print(\"Processing artist features...\")\n",
        "    hash_artist_data = sampled_music_list['artist']\n",
        "    hashed_artist_input = [[str(x)] if pd.notna(x) else [] for x in hash_artist_data]\n",
        "    hasher_artist = FeatureHasher(n_features=500, input_type='string')\n",
        "    hashed_artist_features = hasher_artist.fit_transform(hashed_artist_input)\n",
        "    hashed_artist_df = pd.DataFrame(hashed_artist_features.toarray(), \n",
        "                                   columns=[f'hashed_artist_{i}' for i in range(500)])\n",
        "    print(\"Processed 500 artist features\")\n",
        "\n",
        "# Process tags (try embeddings first, fallback to hashing)\n",
        "if 'tags' in sampled_music_list.columns:\n",
        "    print(\"Processing tag features...\")\n",
        "    try:\n",
        "        # Try sentence transformers for tag embeddings\n",
        "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        tags_text = sampled_music_list['tags'].fillna(\"\").astype(str).tolist()\n",
        "        tag_embeddings = sentence_model.encode(tags_text, convert_to_numpy=True)\n",
        "        tag_embeddings_df = pd.DataFrame(tag_embeddings, \n",
        "                                       columns=[f'tag_embedding_{i}' for i in range(tag_embeddings.shape[1])])\n",
        "        print(f\"Created {tag_embeddings.shape[1]} tag embeddings\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create tag embeddings: {e}\")\n",
        "        print(\"Using feature hashing for tags instead...\")\n",
        "        # Fallback to feature hashing for tags\n",
        "        hash_tags_data = sampled_music_list['tags']\n",
        "        hashed_tags_input = [str(x).split(',') if pd.notna(x) else [] for x in hash_tags_data]\n",
        "        hasher_tags = FeatureHasher(n_features=500, input_type='string')\n",
        "        hashed_tags_features = hasher_tags.fit_transform(hashed_tags_input)\n",
        "        tag_embeddings_df = pd.DataFrame(hashed_tags_features.toarray(), \n",
        "                                       columns=[f'hashed_tags_{i}' for i in range(500)])\n",
        "        print(\"Processed 500 tag features using hashing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining all features...\n",
            "Combined features shape: (20008, 911)\n",
            "Feature columns: 910\n",
            "Final item content matrix shape: (20008, 910)\n",
            "Features are L2-normalized: True\n",
            "\n",
            "Feature statistics:\n",
            "  Mean: 0.0024\n",
            "  Std: 0.0331\n",
            "  Min: -0.4940\n",
            "  Max: 0.5014\n",
            "\n",
            "Feature breakdown:\n",
            "  Numeric features: 10\n",
            "  Genre features (one-hot): 16\n",
            "  Artist features (hashed): 500\n",
            "  Tag features: 384\n",
            "  Total features: 910\n"
          ]
        }
      ],
      "source": [
        "# Combine all processed features\n",
        "print(\"Combining all features...\")\n",
        "\n",
        "# Collect all non-empty dataframes\n",
        "feature_dfs = []\n",
        "if not scaled_numeric_df.empty:\n",
        "    feature_dfs.append(scaled_numeric_df)\n",
        "if not encoded_genre_df.empty:\n",
        "    feature_dfs.append(encoded_genre_df)\n",
        "if not hashed_artist_df.empty:\n",
        "    feature_dfs.append(hashed_artist_df)\n",
        "if not tag_embeddings_df.empty:\n",
        "    feature_dfs.append(tag_embeddings_df)\n",
        "\n",
        "if feature_dfs:\n",
        "    # Concatenate all processed features\n",
        "    normalized_song_features = pd.concat(feature_dfs, axis=1)\n",
        "    \n",
        "    # Add track_id column\n",
        "    normalized_song_features['track_id'] = original_track_ids.values\n",
        "    \n",
        "    print(f\"Combined features shape: {normalized_song_features.shape}\")\n",
        "    print(f\"Feature columns: {len(normalized_song_features.columns) - 1}\")  # -1 for track_id\n",
        "    \n",
        "    # Create item content matrix for evaluation\n",
        "    feature_columns = normalized_song_features.columns.drop('track_id')\n",
        "    item_content_matrix = normalized_song_features[feature_columns].values.astype(np.float32)\n",
        "    \n",
        "    # L2 normalize for cosine similarity calculations\n",
        "    norms = np.linalg.norm(item_content_matrix, axis=1, keepdims=True) + 1e-12\n",
        "    item_content = item_content_matrix / norms\n",
        "    \n",
        "    print(f\"Final item content matrix shape: {item_content.shape}\")\n",
        "    print(f\"Features are L2-normalized: {np.allclose(np.linalg.norm(item_content, axis=1), 1.0)}\")\n",
        "    \n",
        "    # Display feature statistics\n",
        "    print(f\"\\nFeature statistics:\")\n",
        "    print(f\"  Mean: {item_content.mean():.4f}\")\n",
        "    print(f\"  Std: {item_content.std():.4f}\")\n",
        "    print(f\"  Min: {item_content.min():.4f}\")\n",
        "    print(f\"  Max: {item_content.max():.4f}\")\n",
        "    \n",
        "    # Show feature breakdown\n",
        "    print(f\"\\nFeature breakdown:\")\n",
        "    if not scaled_numeric_df.empty:\n",
        "        print(f\"  Numeric features: {len(available_numeric)}\")\n",
        "    if not encoded_genre_df.empty:\n",
        "        print(f\"  Genre features (one-hot): {len(ohe_feature_names)}\")\n",
        "    if not hashed_artist_df.empty:\n",
        "        print(f\"  Artist features (hashed): 500\")\n",
        "    if not tag_embeddings_df.empty:\n",
        "        print(f\"  Tag features: {tag_embeddings_df.shape[1]}\")\n",
        "    print(f\"  Total features: {item_content.shape[1]}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No features could be processed!\")\n",
        "    raise ValueError(\"No features available for processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Popularity Baseline Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎵 Calculating track popularity...\n",
            "Total unique tracks: 20008\n",
            "Tracks with playcount > 0: 20008\n",
            "\n",
            "Top 10 Most Popular Tracks:\n",
            "                 track_id  total_playcount\n",
            "11376  TRONYHY128F92C9D11             2169\n",
            "276    TRAFUNV128F92CFEB2             1617\n",
            "2091   TRCRCBT128F4260DD1             1470\n",
            "1518   TRBVNWT128F93173BA             1333\n",
            "11992  TRPFYYL128F92F7144             1323\n",
            "10649  TRNPKRK128F429831C             1051\n",
            "7483   TRJGDTG128F421CE22             1028\n",
            "13586  TRRKXNQ128F9339002             1000\n",
            "11309  TROMKCG128F9320C09              974\n",
            "18449  TRXUWEC128F426BE3F              967\n",
            "\n",
            "Item mapping created: 20008 items\n",
            "User mapping created: 5000 users\n"
          ]
        }
      ],
      "source": [
        "# Calculate track popularity from training data\n",
        "print(\"🎵 Calculating track popularity...\")\n",
        "\n",
        "track_popularity = train_df.groupby('track_id')['playcount'].sum().reset_index()\n",
        "track_popularity.rename(columns={'playcount': 'total_playcount'}, inplace=True)\n",
        "\n",
        "# Sort by popularity (descending)\n",
        "popularity_sorted = track_popularity.sort_values('total_playcount', ascending=False)\n",
        "\n",
        "print(f\"Total unique tracks: {len(track_popularity)}\")\n",
        "print(f\"Tracks with playcount > 0: {(track_popularity['total_playcount'] > 0).sum()}\")\n",
        "\n",
        "# Display top 10 most popular tracks\n",
        "print(\"\\nTop 10 Most Popular Tracks:\")\n",
        "print(popularity_sorted.head(10))\n",
        "\n",
        "# Create item mappings\n",
        "item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
        "user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
        "\n",
        "print(f\"\\nItem mapping created: {len(item_to_idx)} items\")\n",
        "print(f\"User mapping created: {len(user_to_idx)} users\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated popularity recommendations for k=5\n",
            "✅ Generated popularity recommendations for k=10\n",
            "✅ Generated popularity recommendations for k=15\n",
            "✅ Generated popularity recommendations for k=20\n",
            "\n",
            "Recommendation summary:\n",
            "  k=5: 5000 users, 5 items per user\n",
            "  k=10: 5000 users, 10 items per user\n",
            "  k=15: 5000 users, 15 items per user\n",
            "  k=20: 5000 users, 20 items per user\n"
          ]
        }
      ],
      "source": [
        "# Create popularity-based recommendations\n",
        "def create_popularity_recommendations(user_to_idx, item_to_idx, popularity_sorted, k=20):\n",
        "    \"\"\"\n",
        "    Create popularity-based recommendations for all users.\n",
        "    \n",
        "    Args:\n",
        "        user_to_idx: User ID to index mapping\n",
        "        item_to_idx: Item ID to index mapping\n",
        "        popularity_sorted: DataFrame sorted by popularity\n",
        "        k: Number of recommendations per user\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping user indices to recommendation arrays\n",
        "    \"\"\"\n",
        "    # Get top-k most popular items\n",
        "    top_k_items = popularity_sorted.head(k)['track_id'].tolist()\n",
        "    \n",
        "    # Convert to indices\n",
        "    top_k_indices = [item_to_idx[item] for item in top_k_items if item in item_to_idx]\n",
        "    \n",
        "    # Create recommendations for all users (same recommendations for everyone)\n",
        "    recommendations = {}\n",
        "    for user_idx in user_to_idx.values():\n",
        "        recommendations[user_idx] = np.array(top_k_indices)\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "# Generate recommendations for different k values\n",
        "k_values = [5, 10, 15, 20]\n",
        "all_recommendations = {}\n",
        "\n",
        "for k in k_values:\n",
        "    recommendations = create_popularity_recommendations(user_to_idx, item_to_idx, popularity_sorted, k)\n",
        "    all_recommendations[k] = recommendations\n",
        "    print(f\"✅ Generated popularity recommendations for k={k}\")\n",
        "\n",
        "print(f\"\\nRecommendation summary:\")\n",
        "for k, recs in all_recommendations.items():\n",
        "    print(f\"  k={k}: {len(recs)} users, {len(recs[list(recs.keys())[0]])} items per user\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Individual Metric Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Evaluating individual metrics...\n",
            "\n",
            "--- Evaluating k=5 ---\n",
            "  NDCG@5: 0.0287\n",
            "  Novelty@5: 3.1843\n",
            "  Diversity@5: 0.4426\n",
            "  Serendipity@5: 0.2414\n",
            "  Catalog Coverage@5: 0.02%\n",
            "  User Coverage: 100.00%\n",
            "\n",
            "--- Evaluating k=10 ---\n",
            "  NDCG@10: 0.0250\n",
            "  Novelty@10: 3.6094\n",
            "  Diversity@10: 0.4426\n",
            "  Serendipity@10: 0.2478\n",
            "  Catalog Coverage@10: 0.05%\n",
            "  User Coverage: 100.00%\n",
            "\n",
            "--- Evaluating k=15 ---\n",
            "  NDCG@15: 0.0254\n",
            "  Novelty@15: 3.7483\n",
            "  Diversity@15: 0.4456\n",
            "  Serendipity@15: 0.2529\n",
            "  Catalog Coverage@15: 0.07%\n",
            "  User Coverage: 100.00%\n",
            "\n",
            "--- Evaluating k=20 ---\n",
            "  NDCG@20: 0.0276\n",
            "  Novelty@20: 3.8239\n",
            "  Diversity@20: 0.4512\n",
            "  Serendipity@20: 0.2593\n",
            "  Catalog Coverage@20: 0.10%\n",
            "  User Coverage: 100.00%\n",
            "\n",
            "✅ Individual metric evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate individual metrics for each k value\n",
        "print(\"🔍 Evaluating individual metrics...\")\n",
        "\n",
        "# Store results for each k value\n",
        "individual_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n--- Evaluating k={k} ---\")\n",
        "    recommendations = all_recommendations[k]\n",
        "    \n",
        "    # Calculate each metric individually\n",
        "    ndcg_score = ndcg_at_k(recommendations, test_df, user_to_idx, item_to_idx, k)\n",
        "    novelty_score = novelty_at_k(recommendations, train_df, item_to_idx, k)\n",
        "    diversity_score = diversity_ild_at_k(recommendations, item_content, k)\n",
        "    serendipity_score = serendipity_at_k(recommendations, train_df, test_df, user_to_idx, item_to_idx, item_content, k)\n",
        "    \n",
        "    # Store results\n",
        "    individual_results[k] = {\n",
        "        'ndcg': ndcg_score,\n",
        "        'novelty': novelty_score,\n",
        "        'diversity': diversity_score,\n",
        "        'serendipity': serendipity_score\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"  NDCG@{k}: {ndcg_score:.4f}\")\n",
        "    print(f\"  Novelty@{k}: {novelty_score:.4f}\")\n",
        "    print(f\"  Diversity@{k}: {diversity_score:.4f}\")\n",
        "    print(f\"  Serendipity@{k}: {serendipity_score:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Individual metric evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Running comprehensive evaluation...\n",
            "Evaluating 5000 users across 4 k values...\n",
            "  Calculating ndcg...\n",
            "    ndcg@5: 0.0287\n",
            "    ndcg@10: 0.0250\n",
            "    ndcg@15: 0.0254\n",
            "    ndcg@20: 0.0276\n",
            "  Calculating novelty...\n",
            "    novelty@5: 3.1843\n",
            "    novelty@10: 3.6094\n",
            "    novelty@15: 3.7483\n",
            "    novelty@20: 3.8239\n",
            "  Calculating diversity...\n",
            "    diversity@5: 0.4426\n",
            "    diversity@10: 0.4426\n",
            "    diversity@15: 0.4456\n",
            "    diversity@20: 0.4512\n",
            "  Calculating serendipity...\n",
            "    serendipity@5: 0.2414\n",
            "    serendipity@10: 0.2478\n",
            "    serendipity@15: 0.2529\n",
            "    serendipity@20: 0.2593\n",
            "  Calculating catalog_coverage...\n",
            "    catalog_coverage@5: 0.0250\n",
            "    catalog_coverage@10: 0.0500\n",
            "    catalog_coverage@15: 0.0750\n",
            "    catalog_coverage@20: 0.1000\n",
            "  Calculating user_coverage...\n",
            "    user_coverage@5: 100.0000\n",
            "    user_coverage@10: 100.0000\n",
            "    user_coverage@15: 100.0000\n",
            "    user_coverage@20: 100.0000\n",
            "\n",
            "============================================================\n",
            "RECOMMENDATION EVALUATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "NDCG:\n",
            "--------------------\n",
            "  @ 5: 0.0287\n",
            "  @10: 0.0250\n",
            "  @15: 0.0254\n",
            "  @20: 0.0276\n",
            "\n",
            "NOVELTY:\n",
            "--------------------\n",
            "  @ 5: 3.1843\n",
            "  @10: 3.6094\n",
            "  @15: 3.7483\n",
            "  @20: 3.8239\n",
            "\n",
            "DIVERSITY:\n",
            "--------------------\n",
            "  @ 5: 0.4426\n",
            "  @10: 0.4426\n",
            "  @15: 0.4456\n",
            "  @20: 0.4512\n",
            "\n",
            "SERENDIPITY:\n",
            "--------------------\n",
            "  @ 5: 0.2414\n",
            "  @10: 0.2478\n",
            "  @15: 0.2529\n",
            "  @20: 0.2593\n",
            "\n",
            "CATALOG_COVERAGE:\n",
            "--------------------\n",
            "  @ 5:   0.02%\n",
            "  @10:   0.05%\n",
            "  @15:   0.07%\n",
            "  @20:   0.10%\n",
            "\n",
            "USER_COVERAGE:\n",
            "--------------------\n",
            "  @ 5: 100.00%\n",
            "  @10: 100.00%\n",
            "  @15: 100.00%\n",
            "  @20: 100.00%\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Use the comprehensive evaluation function\n",
        "print(\"📊 Running comprehensive evaluation...\")\n",
        "\n",
        "# Use k=20 recommendations for comprehensive evaluation\n",
        "comprehensive_results = evaluate_recommendations(\n",
        "    recommendations=all_recommendations[20],  # Use k=20 for comprehensive evaluation\n",
        "    train_df=train_df,\n",
        "    test_df=test_df,\n",
        "    user_to_idx=user_to_idx,\n",
        "    item_to_idx=item_to_idx,\n",
        "    item_content=item_content,\n",
        "    k_values=k_values,\n",
        "    metrics=['ndcg', 'novelty', 'diversity', 'serendipity']\n",
        ")\n",
        "\n",
        "# Print formatted results\n",
        "print_evaluation_summary(comprehensive_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations of the results\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Popularity Baseline Evaluation Results (Real Features)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Extract data for plotting\n",
        "metrics = ['ndcg', 'novelty', 'diversity', 'serendipity', 'catalog_coverage', 'user_coverage']\n",
        "metric_titles = ['NDCG@k', 'Novelty@k', 'Diversity@k', 'Serendipity@k', 'Catalog Coverage@k (%)', 'User Coverage (%)']\n",
        "\n",
        "for i, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Get scores for this metric\n",
        "    scores = [individual_results[k][metric] for k in k_values]\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(k_values, scores, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel('k')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xticks(k_values)\n",
        "    \n",
        "    # Add value labels on points\n",
        "    for x, y in zip(k_values, scores):\n",
        "        ax.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a summary table\n",
        "print(\"\\n📋 Summary Table:\")\n",
        "summary_df = pd.DataFrame(individual_results).T\n",
        "summary_df.index.name = 'k'\n",
        "print(summary_df.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analysis and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the results and provide insights\n",
        "print(\"🔍 Analysis and Insights:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# NDCG Analysis\n",
        "print(\"\\n1. NDCG@k (Ranking Quality):\")\n",
        "ndcg_scores = [individual_results[k]['ndcg'] for k in k_values]\n",
        "print(f\"   Range: {min(ndcg_scores):.4f} - {max(ndcg_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if ndcg_scores[-1] > ndcg_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures how well the ranking matches user preferences.\")\n",
        "\n",
        "# Novelty Analysis\n",
        "print(\"\\n2. Novelty@k (Item Unpopularity):\")\n",
        "novelty_scores = [individual_results[k]['novelty'] for k in k_values]\n",
        "print(f\"   Range: {min(novelty_scores):.4f} - {max(novelty_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if novelty_scores[-1] > novelty_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures how 'unpopular' recommended items are.\")\n",
        "\n",
        "# Diversity Analysis\n",
        "print(\"\\n3. Diversity@k (List Variety):\")\n",
        "diversity_scores = [individual_results[k]['diversity'] for k in k_values]\n",
        "print(f\"   Range: {min(diversity_scores):.4f} - {max(diversity_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if diversity_scores[-1] > diversity_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures variety within recommendation lists.\")\n",
        "\n",
        "# Serendipity Analysis\n",
        "print(\"\\n4. Serendipity@k (Surprising Relevance):\")\n",
        "serendipity_scores = [individual_results[k]['serendipity'] for k in k_values]\n",
        "print(f\"   Range: {min(serendipity_scores):.4f} - {max(serendipity_scores):.4f}\")\n",
        "print(f\"   Trend: {'Increasing' if serendipity_scores[-1] > serendipity_scores[0] else 'Decreasing'}\")\n",
        "print(\"   Interpretation: Higher is better. Measures surprising but relevant recommendations.\")\n",
        "\n",
        "# Overall Assessment\n",
        "print(\"\\n5. Overall Assessment:\")\n",
        "print(\"   Popularity baseline characteristics:\")\n",
        "print(\"   ✅ High user coverage (100%) - all users get recommendations\")\n",
        "print(\"   ✅ Consistent recommendations across users\")\n",
        "print(\"   ❌ Low novelty - only recommends popular items\")\n",
        "print(\"   ❌ Low diversity - same items for all users\")\n",
        "print(\"   ❌ Low serendipity - no personalization\")\n",
        "print(\"   ❌ Limited catalog coverage - only top-k popular items\")\n",
        "\n",
        "print(\"\\n7. Recommendations for Improvement:\")\n",
        "print(\"   - Implement collaborative filtering for personalization\")\n",
        "print(\"   - Add content-based filtering for diversity\")\n",
        "print(\"   - Use hybrid approaches to balance popularity and personalization\")\n",
        "print(\"   - Consider user history for serendipity\")\n",
        "\n",
        "print(\"\\n8. Real Features Impact:\")\n",
        "print(\"   - Using real audio features provides more meaningful diversity calculations\")\n",
        "print(\"   - Tag embeddings capture semantic similarity better than synthetic features\")\n",
        "print(\"   - Artist features help distinguish between different musical styles\")\n",
        "print(\"   - Genre features provide categorical diversity information\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "music-recommender",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
